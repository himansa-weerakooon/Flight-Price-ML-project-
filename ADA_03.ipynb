{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnaBGTYU__q0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTYukrGvAewk",
        "outputId": "41694eac-1d94-4c53-c177-19826aa4b6ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Clean_Dataset2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "VSBQJ6F7OLPt",
        "outputId": "30b8efb5-24c3-479c-8518-9353d7c9795e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 100000,\n  \"fields\": [\n    {\n      \"column\": \"airline\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Vistara\",\n          \"GO_FIRST\",\n          \"AirAsia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flight\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1524,\n        \"samples\": [\n          \"6E-898\",\n          \"UK-866\",\n          \"6E-261\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_city\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Mumbai\",\n          \"Hyderabad\",\n          \"Chennai\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"departure_time\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Morning\",\n          \"Early_Morning\",\n          \"Late_Night\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stops\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"one\",\n          \"zero\",\n          \"two_or_more\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arrival_time\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Evening\",\n          \"Night\",\n          \"Early_Morning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"destination_city\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Hyderabad\",\n          \"Delhi\",\n          \"Bangalore\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Economy\",\n          \"Business\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.196976931811347,\n        \"min\": 0.83,\n        \"max\": 49.83,\n        \"num_unique_values\": 453,\n        \"samples\": [\n          32.58,\n          6.67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"days_left\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 1,\n        \"max\": 49,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          8,\n          38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22662,\n        \"min\": 1105,\n        \"max\": 114705,\n        \"num_unique_values\": 8572,\n        \"samples\": [\n          6732,\n          2747\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-aaf6a015-20a4-4d9e-93c7-a229eeb9daec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline</th>\n",
              "      <th>flight</th>\n",
              "      <th>source_city</th>\n",
              "      <th>departure_time</th>\n",
              "      <th>stops</th>\n",
              "      <th>arrival_time</th>\n",
              "      <th>destination_city</th>\n",
              "      <th>class</th>\n",
              "      <th>duration</th>\n",
              "      <th>days_left</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Vistara</td>\n",
              "      <td>UK-970</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>Morning</td>\n",
              "      <td>one</td>\n",
              "      <td>Evening</td>\n",
              "      <td>Hyderabad</td>\n",
              "      <td>Business</td>\n",
              "      <td>8.25</td>\n",
              "      <td>42</td>\n",
              "      <td>60365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vistara</td>\n",
              "      <td>UK-821</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>Early_Morning</td>\n",
              "      <td>one</td>\n",
              "      <td>Night</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Business</td>\n",
              "      <td>17.17</td>\n",
              "      <td>48</td>\n",
              "      <td>51372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GO_FIRST</td>\n",
              "      <td>G8-501</td>\n",
              "      <td>Hyderabad</td>\n",
              "      <td>Early_Morning</td>\n",
              "      <td>one</td>\n",
              "      <td>Night</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>Economy</td>\n",
              "      <td>15.75</td>\n",
              "      <td>45</td>\n",
              "      <td>5947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Vistara</td>\n",
              "      <td>UK-850</td>\n",
              "      <td>Bangalore</td>\n",
              "      <td>Evening</td>\n",
              "      <td>one</td>\n",
              "      <td>Evening</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Business</td>\n",
              "      <td>24.00</td>\n",
              "      <td>10</td>\n",
              "      <td>51817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Vistara</td>\n",
              "      <td>UK-950</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>Night</td>\n",
              "      <td>one</td>\n",
              "      <td>Morning</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Business</td>\n",
              "      <td>11.67</td>\n",
              "      <td>45</td>\n",
              "      <td>62045</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aaf6a015-20a4-4d9e-93c7-a229eeb9daec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aaf6a015-20a4-4d9e-93c7-a229eeb9daec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aaf6a015-20a4-4d9e-93c7-a229eeb9daec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    airline  flight source_city departure_time stops arrival_time  \\\n",
              "0   Vistara  UK-970      Mumbai        Morning   one      Evening   \n",
              "1   Vistara  UK-821      Mumbai  Early_Morning   one        Night   \n",
              "2  GO_FIRST  G8-501   Hyderabad  Early_Morning   one        Night   \n",
              "3   Vistara  UK-850   Bangalore        Evening   one      Evening   \n",
              "4   Vistara  UK-950      Mumbai          Night   one      Morning   \n",
              "\n",
              "  destination_city     class  duration  days_left  price  \n",
              "0        Hyderabad  Business      8.25         42  60365  \n",
              "1            Delhi  Business     17.17         48  51372  \n",
              "2           Mumbai   Economy     15.75         45   5947  \n",
              "3          Kolkata  Business     24.00         10  51817  \n",
              "4          Kolkata  Business     11.67         45  62045  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tNHGS63DPIF",
        "outputId": "d5953a18-9d90-4882-adbb-10b15aea059f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.3.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z44U1NWA0Aqx",
        "outputId": "45ff000d-a887-408b-dbbc-eac46a424a19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== LASSO RESULTS (separate model per class | same split indices) =====\n",
            "   class  n_train  n_test  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2\n",
            "Business    24917    6229 8943.877105 6783.826454  0.522418 8858.198970 6797.977670 0.512340\n",
            " Economy    55083   13771 2559.845811 1709.298177  0.530231 2556.737663 1702.340309 0.528701\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# LASSO (separate model per class)\n",
        "# - Same split across all models via saved indices\n",
        "# - Random split (stratified by class)\n",
        "# - Drop flight\n",
        "# - log1p target transform + expm1 back to price for metrics\n",
        "# Assumes dataset already loaded as df\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# -------------------------\n",
        "# Settings\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "LASSO_ALPHA = 0.001  # light regularization; you can change later\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    \"\"\"Create ONE split indices object you can reuse for other models later.\"\"\"\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    \"\"\"Drop columns, build X/y and log target.\"\"\"\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    y_log = np.log1p(y)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    \"\"\"OneHot categoricals + impute; scale numerics (helps Lasso).\"\"\"\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())  # for Lasso, normal StandardScaler is ideal\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", num_pipe, num_cols),\n",
        "            (\"cat\", cat_pipe, cat_cols),\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    return pre\n",
        "\n",
        "# -------------------------\n",
        "# 1) Make ONE consistent split (reuse later for RF/XGB/CatBoost etc.)\n",
        "# -------------------------\n",
        "train_idx, test_idx = make_split_indices(df)\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -------------------------\n",
        "# 2) Train Lasso separately per class\n",
        "# -------------------------\n",
        "results_rows = {}\n",
        "models = {}\n",
        "\n",
        "for cls in sorted(df[\"class\"].dropna().unique()):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_train, y_train, y_train_log = prepare_X_y(train_sub)\n",
        "    X_test,  y_test,  _           = prepare_X_y(test_sub)\n",
        "\n",
        "    # Ensure same columns order\n",
        "    X_test = X_test.reindex(columns=X_train.columns)\n",
        "\n",
        "    pre = build_preprocessor(X_train)\n",
        "\n",
        "    lasso = Lasso(alpha=LASSO_ALPHA, random_state=SEED, max_iter=20000)\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"pre\", pre),\n",
        "        (\"model\", lasso)\n",
        "    ])\n",
        "\n",
        "    # Fit on log target\n",
        "    pipe.fit(X_train, y_train_log)\n",
        "\n",
        "    # Predict log -> back to price\n",
        "    pred_train = np.expm1(pipe.predict(X_train))\n",
        "    pred_test  = np.expm1(pipe.predict(X_test))\n",
        "\n",
        "    pred_train = np.clip(pred_train, 0, None)\n",
        "    pred_test  = np.clip(pred_test, 0, None)\n",
        "\n",
        "    m_train = compute_metrics(y_train, pred_train)\n",
        "    m_test  = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    models[cls] = pipe\n",
        "    results_rows[cls] = {\n",
        "        \"class\": cls,\n",
        "        \"n_train\": len(X_train),\n",
        "        \"n_test\": len(X_test),\n",
        "        \"train_RMSE\": m_train[\"RMSE\"],\n",
        "        \"train_MAE\": m_train[\"MAE\"],\n",
        "        \"train_R2\": m_train[\"R2\"],\n",
        "        \"test_RMSE\": m_test[\"RMSE\"],\n",
        "        \"test_MAE\": m_test[\"MAE\"],\n",
        "        \"test_R2\": m_test[\"R2\"],\n",
        "    }\n",
        "\n",
        "results_df = pd.DataFrame(list(results_rows.values())).sort_values(\"class\")\n",
        "\n",
        "print(\"\\n===== LASSO RESULTS (separate model per class | same split indices) =====\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# OPTIONAL: save split indices for reuse in your next model code cell\n",
        "# (just keep these variables in memory; or write to disk)\n",
        "# np.save(\"train_idx.npy\", train_idx)\n",
        "# np.save(\"test_idx.npy\", test_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXC8K6OI5pr-",
        "outputId": "0dafa8c4-5a67-4528-c8a7-3fefca12b80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== REGRESSION TREE RESULTS (separate model per class | same split indices) =====\n",
            "   class  n_train  n_test  train_RMSE  train_MAE  train_R2   test_RMSE    test_MAE  test_R2\n",
            "Business    24917    6229  551.880467  43.287274  0.998182 5896.907743 2409.428046 0.783890\n",
            " Economy    55083   13771  108.159008   6.159995  0.999161 2164.860158  904.263907 0.662104\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# REGRESSION TREE (DecisionTreeRegressor) - separate model per class\n",
        "# - Uses the SAME split indices method (create once and reuse)\n",
        "# - Random split (stratified by class)\n",
        "# - Drop flight\n",
        "# - log1p target transform + expm1 back to price for metrics\n",
        "# Assumes dataset already loaded as df\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# -------------------------\n",
        "# Settings\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    \"\"\"Create ONE split indices object you can reuse for other models later.\"\"\"\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    \"\"\"Drop columns, build X/y and log target.\"\"\"\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    y_log = np.log1p(y)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    \"\"\"OneHot categoricals + impute. No scaling needed for trees.\"\"\"\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", num_pipe, num_cols),\n",
        "            (\"cat\", cat_pipe, cat_cols),\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    return pre\n",
        "\n",
        "# -------------------------\n",
        "# 1) Make ONE consistent split (reuse later for RF/XGB/CatBoost etc.)\n",
        "# If you already created train_idx/test_idx in your Lasso cell, you can skip this block.\n",
        "# -------------------------\n",
        "train_idx, test_idx = make_split_indices(df)\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -------------------------\n",
        "# 2) Train Regression Tree separately per class\n",
        "# -------------------------\n",
        "results_rows = []\n",
        "models = {}\n",
        "\n",
        "for cls in sorted(df[\"class\"].dropna().unique()):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_train, y_train, y_train_log = prepare_X_y(train_sub)\n",
        "    X_test,  y_test,  _           = prepare_X_y(test_sub)\n",
        "\n",
        "    # Ensure same columns order\n",
        "    X_test = X_test.reindex(columns=X_train.columns)\n",
        "\n",
        "    pre = build_preprocessor(X_train)\n",
        "\n",
        "    tree = DecisionTreeRegressor(\n",
        "        random_state=SEED,\n",
        "        # no tuning requested; defaults are fine.\n",
        "        # If you want a safer default to reduce overfitting, uncomment:\n",
        "        # max_depth=25,\n",
        "        # min_samples_leaf=10,\n",
        "    )\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"pre\", pre),\n",
        "        (\"model\", tree)\n",
        "    ])\n",
        "\n",
        "    # Fit on log target\n",
        "    pipe.fit(X_train, y_train_log)\n",
        "\n",
        "    # Predict log -> back to price\n",
        "    pred_train = np.expm1(pipe.predict(X_train))\n",
        "    pred_test  = np.expm1(pipe.predict(X_test))\n",
        "\n",
        "    pred_train = np.clip(pred_train, 0, None)\n",
        "    pred_test  = np.clip(pred_test, 0, None)\n",
        "\n",
        "    m_train = compute_metrics(y_train, pred_train)\n",
        "    m_test  = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    models[cls] = pipe\n",
        "    results_rows.append({\n",
        "        \"class\": cls,\n",
        "        \"n_train\": len(X_train),\n",
        "        \"n_test\": len(X_test),\n",
        "        \"train_RMSE\": m_train[\"RMSE\"],\n",
        "        \"train_MAE\": m_train[\"MAE\"],\n",
        "        \"train_R2\": m_train[\"R2\"],\n",
        "        \"test_RMSE\": m_test[\"RMSE\"],\n",
        "        \"test_MAE\": m_test[\"MAE\"],\n",
        "        \"test_R2\": m_test[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_rows).sort_values(\"class\")\n",
        "\n",
        "print(\"\\n===== REGRESSION TREE RESULTS (separate model per class | same split indices) =====\")\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDgPsed86V_o",
        "outputId": "b9ec118e-f061-4bf1-e361-f1d1c5520a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== REGRESSION TREE (regularized) RESULTS =====\n",
            "   class  n_train  n_test  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2\n",
            "Business    24917    6229 8982.757850 6381.839633  0.518257 8979.547533 6451.455562 0.498887\n",
            " Economy    55083   13771 2398.531224 1493.732229  0.587572 2430.627200 1509.063042 0.574048\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    return X, y, np.log1p(y)\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "\n",
        "# IMPORTANT: reuse the SAME split indices you used earlier\n",
        "# If train_idx/test_idx already exist from your Lasso run, keep them and do:\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "rows = []\n",
        "models = {}\n",
        "\n",
        "for cls in sorted(df[\"class\"].dropna().unique()):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_train, y_train, y_train_log = prepare_X_y(train_sub)\n",
        "    X_test,  y_test,  _           = prepare_X_y(test_sub)\n",
        "\n",
        "    X_test = X_test.reindex(columns=X_train.columns)\n",
        "    pre = build_preprocessor(X_train)\n",
        "\n",
        "    # Regularized tree (much less overfit than default)\n",
        "    tree = DecisionTreeRegressor(\n",
        "        random_state=SEED,\n",
        "        max_depth=18,           # limit depth\n",
        "        min_samples_leaf=100,   # key control knob\n",
        "        min_samples_split=200,\n",
        "        max_features=\"sqrt\",    # reduce split greediness\n",
        "        ccp_alpha=1e-6          # pruning (raise if still overfitting)\n",
        "    )\n",
        "\n",
        "    pipe = Pipeline([(\"pre\", pre), (\"model\", tree)])\n",
        "    pipe.fit(X_train, y_train_log)\n",
        "\n",
        "    pred_train = np.clip(np.expm1(pipe.predict(X_train)), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(pipe.predict(X_test)),  0, None)\n",
        "\n",
        "    mt = compute_metrics(y_train, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    models[cls] = pipe\n",
        "    rows.append({\n",
        "        \"class\": cls,\n",
        "        \"n_train\": len(X_train),\n",
        "        \"n_test\": len(X_test),\n",
        "        \"train_RMSE\": mt[\"RMSE\"], \"train_MAE\": mt[\"MAE\"], \"train_R2\": mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"], \"test_MAE\":  ms[\"MAE\"], \"test_R2\":  ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows).sort_values(\"class\")\n",
        "print(\"\\n===== REGRESSION TREE (regularized) RESULTS =====\")\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb9UaNNc8wxC",
        "outputId": "51430905-5e20-46cb-dbb3-26462b31111b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Class=Business | RandomizedSearchCV\n",
            "==============================\n",
            "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Class=Economy | RandomizedSearchCV\n",
            "==============================\n",
            "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# RANDOM FOREST TUNING (per class) with SAME train/test split\n",
        "# - Random split stratified by class (reused via train_idx/test_idx)\n",
        "# - Drop flight\n",
        "# - log1p target transform, expm1 back to price for metrics\n",
        "# - RandomizedSearchCV on TRAIN only (3-fold CV)\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "N_ITER = 25      # increase to 50-100 if you want better search\n",
        "CV_FOLDS = 3\n",
        "N_JOBS = -1\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    y_log = np.log1p(y)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Use same split as before (fallback creates if missing)\n",
        "# -------------------------------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Random Forest tuning space (safe + effective)\n",
        "# -------------------------------------------------\n",
        "param_dist = {\n",
        "    \"model__n_estimators\": [300, 600, 900],\n",
        "    \"model__max_depth\": [None, 15, 25, 35],\n",
        "    \"model__min_samples_split\": [2, 5, 10, 20],\n",
        "    \"model__min_samples_leaf\": [1, 2, 5, 10, 20],\n",
        "    \"model__max_features\": [\"sqrt\", \"log2\", 0.5, 0.8],\n",
        "    \"model__bootstrap\": [True],\n",
        "}\n",
        "\n",
        "rows = []\n",
        "best_models = {}\n",
        "\n",
        "for cls in sorted(df[\"class\"].dropna().unique()):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_train, y_train, y_train_log = prepare_X_y(train_sub)\n",
        "    X_test,  y_test,  _           = prepare_X_y(test_sub)\n",
        "\n",
        "    # Align columns\n",
        "    X_test = X_test.reindex(columns=X_train.columns)\n",
        "\n",
        "    pre = build_preprocessor(X_train)\n",
        "\n",
        "    base_rf = RandomForestRegressor(\n",
        "        random_state=SEED,\n",
        "        n_jobs=N_JOBS\n",
        "    )\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"pre\", pre),\n",
        "        (\"model\", base_rf)\n",
        "    ])\n",
        "\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=pipe,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=N_ITER,\n",
        "        scoring=\"neg_mean_absolute_error\",  # tune for MAE on log scale (still works well)\n",
        "        cv=CV_FOLDS,\n",
        "        random_state=SEED,\n",
        "        n_jobs=N_JOBS,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"Class={cls} | RandomizedSearchCV\")\n",
        "    print(f\"==============================\")\n",
        "    search.fit(X_train, y_train_log)\n",
        "\n",
        "    best_pipe = search.best_estimator_\n",
        "    best_models[cls] = best_pipe\n",
        "\n",
        "    # Evaluate on ORIGINAL price scale\n",
        "    pred_train = np.clip(np.expm1(best_pipe.predict(X_train)), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(best_pipe.predict(X_test)),  0, None)\n",
        "\n",
        "    mt = compute_metrics(y_train, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    rows.append({\n",
        "        \"class\": cls,\n",
        "        \"best_cv_MAE_log\": -search.best_score_,\n",
        "        \"best_params\": search.best_params_,\n",
        "        \"n_train\": len(X_train),\n",
        "        \"n_test\": len(X_test),\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"train_MAE\":  mt[\"MAE\"],\n",
        "        \"train_R2\":   mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"],\n",
        "        \"test_MAE\":   ms[\"MAE\"],\n",
        "        \"test_R2\":    ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows).sort_values(\"class\")\n",
        "\n",
        "print(\"\\n===== TUNED RANDOM FOREST RESULTS (per class | same split) =====\")\n",
        "# best_params is a dict; printing full table can be wide. Print metrics + show params separately if needed.\n",
        "print(results_df.drop(columns=[\"best_params\"]).to_string(index=False))\n",
        "\n",
        "print(\"\\n===== BEST PARAMS PER CLASS =====\")\n",
        "for r in rows:\n",
        "    print(f\"\\nClass: {r['class']}\")\n",
        "    print(r[\"best_params\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "f425a850f3f14bd9ba6ac65b8e20e7e8",
            "7837f94454594f2899350b47e8f4f9f9",
            "547b538b357c4e4fa8b8aa7837a4e77d",
            "c81be900c9c24d4696533ab9f6532873",
            "6af1c5ed2b1041dea0ec1cb1067c14bc",
            "c049d4052747454687ab959dfbfad2f9",
            "9b4ea9e730fe443dadd329e8701e9db1",
            "d30f21d947684d959df444beadef288b",
            "edda336a7d5d41b79f7d32f71013daba",
            "c2935db850d842519ca841a104c83200",
            "afdc10f26fc841a9b7cf10f244d1544a",
            "f0fdd01b5054473f812491dbe7a5c2e8",
            "fcc118fc07ad4f13bee22db333a4f8a8",
            "24c3edd0bb164d21860b83ccba7be66b",
            "a4d9c6e2a21e44eca103a93ab54e7780",
            "14bd71d1505a46feba23aceb46690043",
            "3c8d96b404ad4965889a875ed12cdeaf",
            "5119387e31ae4dbea99c1efe8fa632ed",
            "2e8cfe54727942baabe2698a9304a27b",
            "6b618a2350ab4e38a54bfc1adb3a18e4",
            "c09a62079ad54fedba89d0af1719b6b0",
            "63b0b8ba95bb40369f5c0bb5e1181dca",
            "9d076e4683684e63805abb033a24adbc",
            "659cd46529874fafa8e12434aa16bb62",
            "1fdf92ff53e44ad2bcfb0b4a8ed1aaa4",
            "d9ac1f6e7cf54f70aaa250aafc8682d1",
            "3a478c0b5a1a44859c37aab0ff7ee4c7",
            "981867c3c25841de95529de78f539a3a",
            "3cbe20e720b34d4f92c4911660223e95",
            "0e45ffc00e4349e38060fc10533af924",
            "dbccab523c164707817cee8439b78dcf",
            "17de0d8280c94326b1e4da1ba70b6306",
            "95bfa06b56564ee0b1bdac0437f788bf"
          ]
        },
        "id": "WSst1xQzR39K",
        "outputId": "c87c0762-130d-42bb-ce73-bd4a6f1af10a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f425a850f3f14bd9ba6ac65b8e20e7e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Classes:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0fdd01b5054473f812491dbe7a5c2e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning RF (Business):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d076e4683684e63805abb033a24adbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning RF (Economy):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== FAST-TUNED RANDOM FOREST RESULTS (per class | same split) =====\n",
            "   class  best_val_MAE  n_train  n_test  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2\n",
            "Business   2374.491783    24917    6229 2682.142826 1368.529267  0.957050 4495.417677 2359.475855 0.874407\n",
            " Economy    765.668550    55083   13771  930.481305  419.959235  0.937931 1603.653092  753.196051 0.814585\n",
            "\n",
            "===== BEST PARAMS PER CLASS =====\n",
            "\n",
            "Class: Business\n",
            "{'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 0.8, 'max_depth': None}\n",
            "\n",
            "Class: Economy\n",
            "{'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 0.8, 'max_depth': None}\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# FAST Random Forest tuning (per class) + tqdm progress bar\n",
        "# - SAME train/test split via train_idx/test_idx (reused)\n",
        "# - Inside TRAIN: make a small validation split for fast tuning (no CV)\n",
        "# - Drop flight\n",
        "# - log1p target, expm1 back for metrics\n",
        "# Assumes df is loaded\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, ParameterSampler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15          # split from TRAIN for fast tuning\n",
        "N_TRIALS = 20            # increase to 40 if you want\n",
        "N_JOBS = -1\n",
        "\n",
        "N_EST_TUNE = 250         # small for speed during tuning\n",
        "N_EST_FINAL = 800        # refit best params with more trees\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    y_log = np.log1p(y)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "\n",
        "def eval_on_val(pipe, X_tr, y_tr_log, X_val, y_val):\n",
        "    pipe.fit(X_tr, y_tr_log)\n",
        "    pred_val = np.clip(np.expm1(pipe.predict(X_val)), 0, None)\n",
        "    return float(mean_absolute_error(y_val, pred_val))  # MAE on original scale\n",
        "\n",
        "# -------------------------\n",
        "# Use SAME split as other models\n",
        "# -------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -------------------------\n",
        "# Fast tuning space (smaller = faster)\n",
        "# -------------------------\n",
        "param_space = {\n",
        "    \"max_depth\": [None, 15, 20, 25, 35],\n",
        "    \"min_samples_split\": [2, 5, 10, 20],\n",
        "    \"min_samples_leaf\": [1, 2, 5, 10, 20, 50],\n",
        "    \"max_features\": [\"sqrt\", \"log2\", 0.5, 0.8],\n",
        "}\n",
        "\n",
        "results_rows = []\n",
        "best_models = {}\n",
        "\n",
        "for cls in tqdm(sorted(df[\"class\"].dropna().unique()), desc=\"Classes\"):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_full, y_full, y_full_log = prepare_X_y(train_sub)\n",
        "    X_test, y_test, _          = prepare_X_y(test_sub)\n",
        "\n",
        "    # Align columns\n",
        "    X_test = X_test.reindex(columns=X_full.columns)\n",
        "\n",
        "    # Split train into train/val (FAST tuning)\n",
        "    X_tr, X_val, y_tr, y_val, y_tr_log, y_val_log = train_test_split(\n",
        "        X_full, y_full, y_full_log,\n",
        "        test_size=VAL_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    pre = build_preprocessor(X_full)\n",
        "\n",
        "    # Sample parameter combos\n",
        "    sampler = list(ParameterSampler(param_space, n_iter=N_TRIALS, random_state=SEED))\n",
        "\n",
        "    best_val_mae = float(\"inf\")\n",
        "    best_params = None\n",
        "\n",
        "    pbar = tqdm(sampler, desc=f\"Tuning RF ({cls})\", leave=False)\n",
        "    for params in pbar:\n",
        "        rf = RandomForestRegressor(\n",
        "            n_estimators=N_EST_TUNE,     # small for speed\n",
        "            random_state=SEED,\n",
        "            n_jobs=N_JOBS,\n",
        "            bootstrap=True,\n",
        "            **params\n",
        "        )\n",
        "        pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", rf)])\n",
        "\n",
        "        val_mae = eval_on_val(pipe, X_tr, y_tr_log, X_val, y_val)\n",
        "\n",
        "        if val_mae < best_val_mae:\n",
        "            best_val_mae = val_mae\n",
        "            best_params = params\n",
        "\n",
        "        pbar.set_postfix(best_val_MAE=f\"{best_val_mae:.2f}\")\n",
        "\n",
        "    # Refit BEST on FULL TRAIN (more trees), evaluate train & test\n",
        "    best_rf = RandomForestRegressor(\n",
        "        n_estimators=N_EST_FINAL,\n",
        "        random_state=SEED,\n",
        "        n_jobs=N_JOBS,\n",
        "        bootstrap=True,\n",
        "        **best_params\n",
        "    )\n",
        "    best_pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", best_rf)])\n",
        "    best_pipe.fit(X_full, y_full_log)\n",
        "\n",
        "    pred_train = np.clip(np.expm1(best_pipe.predict(X_full)), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(best_pipe.predict(X_test)), 0, None)\n",
        "\n",
        "    mt = compute_metrics(y_full, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    best_models[cls] = best_pipe\n",
        "\n",
        "    results_rows.append({\n",
        "        \"class\": cls,\n",
        "        \"best_val_MAE\": best_val_mae,\n",
        "        \"best_params\": best_params,\n",
        "        \"n_train\": len(X_full),\n",
        "        \"n_test\": len(X_test),\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"train_MAE\":  mt[\"MAE\"],\n",
        "        \"train_R2\":   mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"],\n",
        "        \"test_MAE\":   ms[\"MAE\"],\n",
        "        \"test_R2\":    ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_rows).sort_values(\"class\")\n",
        "\n",
        "print(\"\\n===== FAST-TUNED RANDOM FOREST RESULTS (per class | same split) =====\")\n",
        "print(results_df.drop(columns=[\"best_params\"]).to_string(index=False))\n",
        "\n",
        "print(\"\\n===== BEST PARAMS PER CLASS =====\")\n",
        "for r in results_rows:\n",
        "    print(f\"\\nClass: {r['class']}\")\n",
        "    print(r[\"best_params\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "716e557efc6a48dcb80c823826cc6e4d",
            "03b20c42231548c38f09fa04d438be66",
            "1729252d25394908a533c719d194a9b7",
            "aa75f9386f52403bba4a0e06e9447a5f",
            "dbffe8048fb0476eb8868430668d7c3a",
            "b8084aa5f5134a6296344e664ad549d8",
            "c9cc4b734e7d4885885d62050a857019",
            "88da83417dc14db3a8a39595988cfcab",
            "5fe07272b86f4846aafc39e22925e1c4",
            "5be94ef36c064fd1bb92bbcebe686ab9",
            "4b6c3d61bb8e44c197a530a45b9c0826",
            "756b933b50c4476fa0c1153113e9840e",
            "201b9a25162445b396a7255977d2cfd7",
            "2f6d4aba7d514e6796e5835e672dc881",
            "2517c8c66bf24b82ae3bd7422a6b9297",
            "888bbc95fc4c47d1b5b15da2555a1f8d",
            "8f899828f8834fd5a3d23b5b9c7c4743",
            "dad6c07d06c046c3b85139eb1dc78625",
            "1397f7d37f4e47f2b9d14d6d5c5a45b2",
            "348c84be52c848cd94d9ac0effb2a042",
            "854bd6a36080451fa45f1d91d11585b5",
            "60998dab3e9044ad89fb67a979ab82d3"
          ]
        },
        "id": "F43-Y6nexBi_",
        "outputId": "d77994a6-1794-4937-ddd8-a672aa2fc586"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "716e557efc6a48dcb80c823826cc6e4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Classes:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "756b933b50c4476fa0c1153113e9840e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning XGB (Business):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "XGBModel.fit() got an unexpected keyword argument 'eval_metric'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3051005187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m         )\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         model.fit(\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: XGBModel.fit() got an unexpected keyword argument 'eval_metric'"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# FAST XGBOOST TUNING (per class) + tqdm progress bar + EARLY STOPPING\n",
        "# - SAME train/test split via train_idx/test_idx (fair comparison)\n",
        "# - Inside TRAIN: train/val split for early stopping (no CV -> fast)\n",
        "# - Drop flight\n",
        "# - log1p target, expm1 back for metrics\n",
        "# Assumes df is already loaded\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, ParameterSampler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from scipy import sparse\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "N_TRIALS = 20            # increase to 40+ for better search (slower)\n",
        "N_ESTIMATORS = 20000     # high, early stopping will cut it\n",
        "EARLY_STOP = 300         # rounds w/o improvement on val before stopping\n",
        "N_JOBS = -1\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    y_log = np.log1p(y)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "    return pre\n",
        "\n",
        "def xgb_predict_best(model: XGBRegressor, X_mat):\n",
        "    \"\"\"\n",
        "    Predict using best_iteration if available.\n",
        "    Handles different xgboost versions.\n",
        "    \"\"\"\n",
        "    # Newer xgboost: iteration_range works\n",
        "    best_it = getattr(model, \"best_iteration\", None)\n",
        "    if best_it is not None and best_it is not False:\n",
        "        try:\n",
        "            return model.predict(X_mat, iteration_range=(0, best_it + 1))\n",
        "        except TypeError:\n",
        "            pass\n",
        "\n",
        "    # Older xgboost: ntree_limit / best_ntree_limit\n",
        "    best_ntree_limit = getattr(model, \"best_ntree_limit\", None)\n",
        "    if best_ntree_limit is not None:\n",
        "        try:\n",
        "            return model.predict(X_mat, ntree_limit=best_ntree_limit)\n",
        "        except TypeError:\n",
        "            pass\n",
        "\n",
        "    # Fallback\n",
        "    return model.predict(X_mat)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Use SAME split as other models (fallback creates if missing)\n",
        "# -------------------------------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Parameter space (FAST + effective)\n",
        "# -------------------------------------------------\n",
        "param_space = {\n",
        "    \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
        "    \"max_depth\": [4, 6, 8, 10],\n",
        "    \"min_child_weight\": [1, 3, 5, 10],\n",
        "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"reg_lambda\": [0.5, 1.0, 2.0, 5.0, 10.0],\n",
        "    \"reg_alpha\": [0.0, 0.1, 0.5, 1.0],\n",
        "    \"gamma\": [0.0, 0.05, 0.1, 0.2],\n",
        "}\n",
        "\n",
        "results_rows = []\n",
        "best_models = {}\n",
        "\n",
        "for cls in tqdm(sorted(df[\"class\"].dropna().unique()), desc=\"Classes\"):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_full, y_full, y_full_log = prepare_X_y(train_sub)\n",
        "    X_test_df, y_test, _       = prepare_X_y(test_sub)\n",
        "\n",
        "    # Align columns\n",
        "    X_test_df = X_test_df.reindex(columns=X_full.columns)\n",
        "\n",
        "    # Split TRAIN -> train/val for early stopping\n",
        "    X_tr_df, X_val_df, y_tr, y_val, y_tr_log, y_val_log = train_test_split(\n",
        "        X_full, y_full, y_full_log,\n",
        "        test_size=VAL_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Fit preprocessor ONLY on training portion (no leakage)\n",
        "    pre = build_preprocessor(X_tr_df)\n",
        "    X_tr = pre.fit_transform(X_tr_df)\n",
        "    X_val = pre.transform(X_val_df)\n",
        "    X_full_mat = pre.transform(X_full)      # for final refit scoring\n",
        "    X_test = pre.transform(X_test_df)\n",
        "\n",
        "    # Ensure CSR for speed\n",
        "    if sparse.issparse(X_tr):  X_tr = X_tr.tocsr()\n",
        "    if sparse.issparse(X_val): X_val = X_val.tocsr()\n",
        "    if sparse.issparse(X_full_mat): X_full_mat = X_full_mat.tocsr()\n",
        "    if sparse.issparse(X_test): X_test = X_test.tocsr()\n",
        "\n",
        "    sampler = list(ParameterSampler(param_space, n_iter=N_TRIALS, random_state=SEED))\n",
        "\n",
        "    best_val_mae = float(\"inf\")\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    pbar = tqdm(sampler, desc=f\"Tuning XGB ({cls})\", leave=False)\n",
        "    for params in pbar:\n",
        "        model = XGBRegressor(\n",
        "            objective=\"reg:squarederror\",\n",
        "            n_estimators=N_ESTIMATORS,\n",
        "            tree_method=\"hist\",      # fast CPU training\n",
        "            n_jobs=N_JOBS,\n",
        "            random_state=SEED,\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        model.fit(\n",
        "            X_tr, y_tr_log,\n",
        "            eval_set=[(X_val, y_val_log)],\n",
        "            eval_metric=\"rmse\",\n",
        "            verbose=False,\n",
        "            early_stopping_rounds=EARLY_STOP\n",
        "        )\n",
        "\n",
        "        # Evaluate on ORIGINAL scale using best iteration\n",
        "        val_pred_log = xgb_predict_best(model, X_val)\n",
        "        val_pred = np.clip(np.expm1(val_pred_log), 0, None)\n",
        "        val_mae = float(mean_absolute_error(y_val, val_pred))\n",
        "\n",
        "        if val_mae < best_val_mae:\n",
        "            best_val_mae = val_mae\n",
        "            best_params = params\n",
        "            best_model = model\n",
        "\n",
        "        pbar.set_postfix(best_val_MAE=f\"{best_val_mae:.2f}\", best_iter=getattr(best_model, \"best_iteration\", None))\n",
        "\n",
        "    # Refit best model on FULL TRAIN (still use early stopping? faster to just use best iteration)\n",
        "    # We'll train with n_estimators = best_iteration+1 to lock in the chosen complexity.\n",
        "    best_it = getattr(best_model, \"best_iteration\", None)\n",
        "    final_n = (best_it + 1) if best_it is not None else 2000\n",
        "\n",
        "    final_model = XGBRegressor(\n",
        "        objective=\"reg:squarederror\",\n",
        "        n_estimators=final_n,\n",
        "        tree_method=\"hist\",\n",
        "        n_jobs=N_JOBS,\n",
        "        random_state=SEED,\n",
        "        **best_params\n",
        "    )\n",
        "    final_model.fit(X_full_mat, y_full_log, verbose=False)\n",
        "\n",
        "    # Train/Test predictions (original price scale)\n",
        "    pred_train = np.clip(np.expm1(final_model.predict(X_full_mat)), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(final_model.predict(X_test)),     0, None)\n",
        "\n",
        "    mt = compute_metrics(y_full, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    best_models[cls] = {\"pre\": pre, \"model\": final_model}\n",
        "\n",
        "    results_rows.append({\n",
        "        \"class\": cls,\n",
        "        \"best_val_MAE\": best_val_mae,\n",
        "        \"best_iter\": final_n,\n",
        "        \"best_params\": best_params,\n",
        "        \"n_train\": len(X_full),\n",
        "        \"n_test\": len(X_test_df),\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"train_MAE\":  mt[\"MAE\"],\n",
        "        \"train_R2\":   mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"],\n",
        "        \"test_MAE\":   ms[\"MAE\"],\n",
        "        \"test_R2\":    ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_rows).sort_values(\"class\")\n",
        "\n",
        "print(\"\\n===== FAST-TUNED XGBOOST RESULTS (per class | same split | early stopping) =====\")\n",
        "print(results_df.drop(columns=[\"best_params\"]).to_string(index=False))\n",
        "\n",
        "print(\"\\n===== BEST PARAMS PER CLASS =====\")\n",
        "for r in results_rows:\n",
        "    print(f\"\\nClass: {r['class']}\")\n",
        "    print(r[\"best_params\"])\n",
        "    print(f\"Final n_estimators used: {r['best_iter']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "cbdbb6d287fd45a294dff7205d14eec5",
            "574867863b0246dd9cb17bba949fd0bc",
            "f75c2ada78c84dcb84327f150d9384ec",
            "0d74dbf9e0224c69ac482c924aec9082",
            "822facdfac154567882a31e4b462d3d2",
            "139dfa067b414ad1bd0b9d76bb1db804",
            "1ef46e326c06417493675bc287e5c7bd",
            "76f4f8bcd0b149c2ac0dbb2c8a215c1d",
            "18ca0697296f4c92bed15b0389040202",
            "82775c9902514756bddeedb6fa401939",
            "a73fa34638334fe0be4b1f4ba8b87e44",
            "3d8d0549356a4696a9a3f098f816f072",
            "a6e159f36a89482ca1d3c81e1969e38f",
            "679c0b662bac49728ec1620b920c1d67",
            "703a2c66faf94e069120cb4ee2625d2a",
            "75882d8a378141719996c5fe93cbb209",
            "58fdc30a1fa6491a9e09ba0a1eb02339",
            "aff2b4b5d909455681a74358b2fb5d44",
            "ee7b965a65934f76a1fd811274441126",
            "424fc7f31db74a338f6eb33a61957efd",
            "8137899e21b146db8ada2c49d2a23ebc",
            "acd5795ac9da4eafa8b7d29ce414843d",
            "9c21de8bd1d140b3899f5b3f0cdbd135",
            "4b85b81c9fe04dfca64a65bb9a574818",
            "c76b51802eeb4e21bb5054e2444e1eb3",
            "003726a3011947e688f38b67877149b9",
            "41c95e42610f4bea8bf1ea5f9a086f48",
            "9eb11cfd151245ae921294a0f06da94b",
            "66832a701a9f48f0954da0e88c2af4e9",
            "4eb9fc8704d344258d9718a698912d12",
            "f8635c2ef6174fb587168d77b3a22570",
            "da55a9aadb374fbaa894fd3cf5137784",
            "e5e1d6fed04a4a59a160ce2b0f3e80f8"
          ]
        },
        "id": "RGuxfU329syd",
        "outputId": "c59bb02b-5404-4d53-b370-f76f7acb4cde"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbdbb6d287fd45a294dff7205d14eec5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Classes:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d8d0549356a4696a9a3f098f816f072",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning XGB (Business):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c21de8bd1d140b3899f5b3f0cdbd135",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning XGB (Economy):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== FAST-TUNED XGBOOST RESULTS (per class | same split | early stopping) =====\n",
            "   class  best_val_MAE  best_iter_used  n_train  n_test  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2\n",
            "Business   2667.949030            2000    24917    6229 3449.036649 2159.197862  0.928978 4589.464134 2849.827854 0.869097\n",
            " Economy    776.476411            2000    55083   13771 1091.525733  596.856480  0.914587 1535.801493  807.679269 0.829943\n",
            "\n",
            "===== BEST PARAMS PER CLASS =====\n",
            "\n",
            "Class: Business\n",
            "{'subsample': 0.8, 'reg_lambda': 2.0, 'reg_alpha': 0.5, 'min_child_weight': 3, 'max_depth': 8, 'learning_rate': 0.03, 'gamma': 0.0, 'colsample_bytree': 0.7}\n",
            "Final n_estimators used: 2000\n",
            "\n",
            "Class: Economy\n",
            "{'subsample': 0.8, 'reg_lambda': 2.0, 'reg_alpha': 0.5, 'min_child_weight': 3, 'max_depth': 8, 'learning_rate': 0.03, 'gamma': 0.0, 'colsample_bytree': 0.7}\n",
            "Final n_estimators used: 2000\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# FAST XGBOOST TUNING (per class) + tqdm progress bar + EARLY STOPPING\n",
        "# FIXED: eval_metric moved to constructor (not model.fit)\n",
        "# - SAME train/test split via train_idx/test_idx (fair comparison)\n",
        "# - Inside TRAIN: train/val split for early stopping (fast, no CV)\n",
        "# - Drop flight\n",
        "# - log1p target, expm1 back for metrics\n",
        "# Assumes df is already loaded\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, ParameterSampler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from scipy import sparse\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "N_TRIALS = 20            # increase to 40+ for better search (slower)\n",
        "N_ESTIMATORS = 20000     # high; early stopping will stop earlier\n",
        "EARLY_STOP = 300         # rounds without improvement\n",
        "\n",
        "N_JOBS = -1\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    y_log = np.log1p(y)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "    return pre\n",
        "\n",
        "def fit_with_early_stopping(model, X_tr, y_tr, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Version-safe early stopping:\n",
        "    - Try early_stopping_rounds (works in many versions)\n",
        "    - If not supported, try callback EarlyStopping\n",
        "    - If neither works, fit without early stopping (still runs)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=False,\n",
        "            early_stopping_rounds=EARLY_STOP\n",
        "        )\n",
        "        return model\n",
        "    except TypeError:\n",
        "        try:\n",
        "            from xgboost.callback import EarlyStopping\n",
        "            model.fit(\n",
        "                X_tr, y_tr,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False,\n",
        "                callbacks=[EarlyStopping(rounds=EARLY_STOP, save_best=True)]\n",
        "            )\n",
        "            return model\n",
        "        except Exception:\n",
        "            model.fit(X_tr, y_tr)\n",
        "            return model\n",
        "\n",
        "def predict_best(model, X):\n",
        "    \"\"\"\n",
        "    Predict using the best iteration if available (version-safe).\n",
        "    \"\"\"\n",
        "    best_it = getattr(model, \"best_iteration\", None)\n",
        "    if best_it is not None and best_it is not False:\n",
        "        try:\n",
        "            return model.predict(X, iteration_range=(0, best_it + 1))\n",
        "        except TypeError:\n",
        "            pass\n",
        "\n",
        "    best_ntree_limit = getattr(model, \"best_ntree_limit\", None)\n",
        "    if best_ntree_limit is not None:\n",
        "        try:\n",
        "            return model.predict(X, ntree_limit=best_ntree_limit)\n",
        "        except TypeError:\n",
        "            pass\n",
        "\n",
        "    return model.predict(X)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Use SAME split as other models (fallback creates if missing)\n",
        "# -------------------------------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Parameter space (FAST + effective)\n",
        "# -------------------------------------------------\n",
        "param_space = {\n",
        "    \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
        "    \"max_depth\": [4, 6, 8, 10],\n",
        "    \"min_child_weight\": [1, 3, 5, 10],\n",
        "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"reg_lambda\": [0.5, 1.0, 2.0, 5.0, 10.0],\n",
        "    \"reg_alpha\": [0.0, 0.1, 0.5, 1.0],\n",
        "    \"gamma\": [0.0, 0.05, 0.1, 0.2],\n",
        "}\n",
        "\n",
        "results_rows = []\n",
        "best_models = {}\n",
        "\n",
        "for cls in tqdm(sorted(df[\"class\"].dropna().unique()), desc=\"Classes\"):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_full_df, y_full, y_full_log = prepare_X_y(train_sub)\n",
        "    X_test_df, y_test, _          = prepare_X_y(test_sub)\n",
        "\n",
        "    X_test_df = X_test_df.reindex(columns=X_full_df.columns)\n",
        "\n",
        "    # TRAIN -> train/val split for early stopping\n",
        "    X_tr_df, X_val_df, y_tr, y_val, y_tr_log, y_val_log = train_test_split(\n",
        "        X_full_df, y_full, y_full_log,\n",
        "        test_size=VAL_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Fit preprocessor ONLY on training portion (avoid leakage)\n",
        "    pre = build_preprocessor(X_tr_df)\n",
        "    X_tr = pre.fit_transform(X_tr_df)\n",
        "    X_val = pre.transform(X_val_df)\n",
        "\n",
        "    X_full = pre.transform(X_full_df)\n",
        "    X_test = pre.transform(X_test_df)\n",
        "\n",
        "    # Make CSR sparse for speed\n",
        "    if sparse.issparse(X_tr):   X_tr = X_tr.tocsr()\n",
        "    if sparse.issparse(X_val):  X_val = X_val.tocsr()\n",
        "    if sparse.issparse(X_full): X_full = X_full.tocsr()\n",
        "    if sparse.issparse(X_test): X_test = X_test.tocsr()\n",
        "\n",
        "    sampler = list(ParameterSampler(param_space, n_iter=N_TRIALS, random_state=SEED))\n",
        "\n",
        "    best_val_mae = float(\"inf\")\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    pbar = tqdm(sampler, desc=f\"Tuning XGB ({cls})\", leave=False)\n",
        "    for params in pbar:\n",
        "        # FIX: eval_metric goes here (constructor), NOT in .fit()\n",
        "        model = XGBRegressor(\n",
        "            objective=\"reg:squarederror\",\n",
        "            eval_metric=\"rmse\",\n",
        "            n_estimators=N_ESTIMATORS,\n",
        "            tree_method=\"hist\",\n",
        "            n_jobs=N_JOBS,\n",
        "            random_state=SEED,\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        model = fit_with_early_stopping(model, X_tr, y_tr_log, X_val, y_val_log)\n",
        "\n",
        "        # Evaluate on ORIGINAL price scale\n",
        "        val_pred_log = predict_best(model, X_val)\n",
        "        val_pred = np.clip(np.expm1(val_pred_log), 0, None)\n",
        "        val_mae = float(mean_absolute_error(y_val, val_pred))\n",
        "\n",
        "        if val_mae < best_val_mae:\n",
        "            best_val_mae = val_mae\n",
        "            best_params = params\n",
        "            best_model = model\n",
        "\n",
        "        pbar.set_postfix(best_val_MAE=f\"{best_val_mae:.2f}\", best_iter=getattr(best_model, \"best_iteration\", None))\n",
        "\n",
        "    # Final refit on FULL TRAIN using best complexity\n",
        "    best_it = getattr(best_model, \"best_iteration\", None)\n",
        "    final_n = (best_it + 1) if best_it is not None and best_it is not False else 2000\n",
        "\n",
        "    final_model = XGBRegressor(\n",
        "        objective=\"reg:squarederror\",\n",
        "        eval_metric=\"rmse\",\n",
        "        n_estimators=final_n,\n",
        "        tree_method=\"hist\",\n",
        "        n_jobs=N_JOBS,\n",
        "        random_state=SEED,\n",
        "        **best_params\n",
        "    )\n",
        "    final_model.fit(X_full, y_full_log, verbose=False)\n",
        "\n",
        "    pred_train = np.clip(np.expm1(final_model.predict(X_full)), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(final_model.predict(X_test)), 0, None)\n",
        "\n",
        "    mt = compute_metrics(y_full, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    best_models[cls] = {\"pre\": pre, \"model\": final_model}\n",
        "\n",
        "    results_rows.append({\n",
        "        \"class\": cls,\n",
        "        \"best_val_MAE\": best_val_mae,\n",
        "        \"best_iter_used\": final_n,\n",
        "        \"best_params\": best_params,\n",
        "        \"n_train\": len(X_full_df),\n",
        "        \"n_test\": len(X_test_df),\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"train_MAE\":  mt[\"MAE\"],\n",
        "        \"train_R2\":   mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"],\n",
        "        \"test_MAE\":   ms[\"MAE\"],\n",
        "        \"test_R2\":    ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_rows).sort_values(\"class\")\n",
        "\n",
        "print(\"\\n===== FAST-TUNED XGBOOST RESULTS (per class | same split | early stopping) =====\")\n",
        "print(results_df.drop(columns=[\"best_params\"]).to_string(index=False))\n",
        "\n",
        "print(\"\\n===== BEST PARAMS PER CLASS =====\")\n",
        "for r in results_rows:\n",
        "    print(f\"\\nClass: {r['class']}\")\n",
        "    print(r[\"best_params\"])\n",
        "    print(f\"Final n_estimators used: {r['best_iter_used']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "0c869b50e74446238decc6e3228910d3",
            "fc2214f96c854dbd8452c47a28b37074",
            "2eb166005ac54b4f89b80080b3cbb29a",
            "5b7ae5d7f5d24adea734db352b2e5366",
            "fe150274ddbd448cb0d8a761c6f4089d",
            "0d5f616126424bac9b08aa4ebbf55423",
            "c657483bb18a44bdb22faf86a2054919",
            "baaee17ca22f4dba98bbcdfa599d7723",
            "5cf9cff500fd48b3a1f21a1deb0862e4",
            "294f820418b74671ba3a1de7f5633a89",
            "848cc1eddade4e308e07e126040a7910",
            "969db3f45075404aa9c10ee9c3a244a5",
            "3f94dcd8259649d7bb3b79406cf3be0d",
            "4e1f6bb4fe8942528e633c98f5e08e6d",
            "10caec34745146f99238bb9a204c6b82",
            "71cad5977c0e4b62bf9fc0cca977556d",
            "8e0d8d6aa8e04a35bd6e04fa64f1cf04",
            "dd7dbc3a616145a5917e94424f2b8885",
            "f4af904aa38745a9a2020abce6955a43",
            "4f07faf097ab47a7ac2a6622524bb903",
            "57e9906d6217411482d75e754c643ccf",
            "08815df927b44d23899393653f8f00b9",
            "61a077b11f894769b8894e4c413801f0",
            "ab7984dc7ee3499eb4c8dff21a24144b",
            "ff76d5841f604b818c02a151dc62c40a",
            "9994fbbd7a804d9d82814e05c5ed0037",
            "534b897b9ad645bb8e3aa22961b37095",
            "33890518cf2b4f67bf48ab4418222242",
            "88be339d248a42c88eecd246d3b7ecdc",
            "68556ea9f95848029447fa9988ecb5e4",
            "5addf15222b54c8a843e8792a2e69cf8",
            "cb7fe3857f05479b914fe56b8dbeea9b",
            "83354fe12d9f490891ce87269ba79dec"
          ]
        },
        "id": "vXt95f5z-e_w",
        "outputId": "93bcd86d-677b-4fb2-a166-834c58e8f092"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c869b50e74446238decc6e3228910d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Classes:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "969db3f45075404aa9c10ee9c3a244a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning XGB (Business):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61a077b11f894769b8894e4c413801f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning XGB (Economy):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== FAST-TUNED XGBOOST RESULTS (early stopping truly applied) =====\n",
            "   class  best_val_MAE best_iteration  n_train  n_test  train_RMSE   train_MAE  train_R2  test_RMSE    test_MAE  test_R2\n",
            "Business   2591.474945           None    24917    6229 2097.919166 1035.972023  0.973723 4600.68789 2711.654226 0.868456\n",
            " Economy    741.512990           None    55083   13771  742.238822  343.941807  0.960505 1508.02142  743.897248 0.836040\n",
            "\n",
            "===== BEST PARAMS PER CLASS =====\n",
            "\n",
            "Class: Business\n",
            "best_iteration: None\n",
            "{'subsample': 0.7, 'reg_lambda': 10.0, 'reg_alpha': 0.1, 'min_child_weight': 3, 'max_depth': 10, 'learning_rate': 0.02, 'gamma': 0.0, 'colsample_bytree': 0.7}\n",
            "\n",
            "Class: Economy\n",
            "best_iteration: None\n",
            "{'subsample': 0.7, 'reg_lambda': 10.0, 'reg_alpha': 0.1, 'min_child_weight': 3, 'max_depth': 10, 'learning_rate': 0.02, 'gamma': 0.0, 'colsample_bytree': 0.7}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, ParameterSampler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from scipy import sparse\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "N_TRIALS = 20\n",
        "N_ESTIMATORS = 20000\n",
        "EARLY_STOP = 200     # a bit smaller helps it trigger sooner\n",
        "N_JOBS = -1\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def add_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    # add interaction features (BIG accuracy boost often)\n",
        "    X = add_interactions(X)\n",
        "\n",
        "    y_log = np.log1p(y)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "\n",
        "def fit_with_early_stopping(model, X_tr, y_tr, X_val, y_val):\n",
        "    # Version-safe early stopping\n",
        "    try:\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=False,\n",
        "            early_stopping_rounds=EARLY_STOP\n",
        "        )\n",
        "        return model\n",
        "    except TypeError:\n",
        "        try:\n",
        "            from xgboost.callback import EarlyStopping\n",
        "            model.fit(\n",
        "                X_tr, y_tr,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False,\n",
        "                callbacks=[EarlyStopping(rounds=EARLY_STOP, save_best=True)]\n",
        "            )\n",
        "            return model\n",
        "        except Exception:\n",
        "            model.fit(X_tr, y_tr)\n",
        "            return model\n",
        "\n",
        "def get_best_iteration(model):\n",
        "    \"\"\"\n",
        "    Robustly extract best iteration from sklearn wrapper or booster attrs.\n",
        "    Returns int or None.\n",
        "    \"\"\"\n",
        "    # common in newer versions\n",
        "    bi = getattr(model, \"best_iteration\", None)\n",
        "    if bi is not None and bi is not False:\n",
        "        return int(bi)\n",
        "\n",
        "    booster = model.get_booster()\n",
        "    # stored as string attr in many builds\n",
        "    bi_attr = booster.attr(\"best_iteration\")\n",
        "    if bi_attr is not None:\n",
        "        try:\n",
        "            return int(bi_attr)\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    return None\n",
        "\n",
        "def predict_with_best_iteration(model, X):\n",
        "    best_it = get_best_iteration(model)\n",
        "    if best_it is not None:\n",
        "        # xgboost >=1.6\n",
        "        try:\n",
        "            return model.predict(X, iteration_range=(0, best_it + 1))\n",
        "        except TypeError:\n",
        "            pass\n",
        "\n",
        "    # fallback\n",
        "    return model.predict(X)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# SAME split as your other models\n",
        "# -------------------------------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Fast parameter space\n",
        "# -------------------------------------------------\n",
        "param_space = {\n",
        "    \"learning_rate\": [0.02, 0.03, 0.05, 0.08],\n",
        "    \"max_depth\": [4, 6, 8, 10],\n",
        "    \"min_child_weight\": [1, 3, 5, 10],\n",
        "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"reg_lambda\": [1.0, 2.0, 5.0, 10.0],\n",
        "    \"reg_alpha\": [0.0, 0.1, 0.5, 1.0],\n",
        "    \"gamma\": [0.0, 0.05, 0.1, 0.2],\n",
        "}\n",
        "\n",
        "results_rows = []\n",
        "best_models = {}\n",
        "\n",
        "for cls in tqdm(sorted(df[\"class\"].dropna().unique()), desc=\"Classes\"):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_full_df, y_full, y_full_log = prepare_X_y(train_sub)\n",
        "    X_test_df, y_test, _          = prepare_X_y(test_sub)\n",
        "\n",
        "    # align columns\n",
        "    X_test_df = X_test_df.reindex(columns=X_full_df.columns)\n",
        "\n",
        "    # train/val split inside training for early stopping\n",
        "    X_tr_df, X_val_df, y_tr, y_val, y_tr_log, y_val_log = train_test_split(\n",
        "        X_full_df, y_full, y_full_log,\n",
        "        test_size=VAL_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # preprocess fit on training portion only (no leakage)\n",
        "    pre = build_preprocessor(X_tr_df)\n",
        "    X_tr = pre.fit_transform(X_tr_df)\n",
        "    X_val = pre.transform(X_val_df)\n",
        "\n",
        "    X_full = pre.transform(X_full_df)\n",
        "    X_test = pre.transform(X_test_df)\n",
        "\n",
        "    # CSR for speed\n",
        "    if sparse.issparse(X_tr):   X_tr = X_tr.tocsr()\n",
        "    if sparse.issparse(X_val):  X_val = X_val.tocsr()\n",
        "    if sparse.issparse(X_full): X_full = X_full.tocsr()\n",
        "    if sparse.issparse(X_test): X_test = X_test.tocsr()\n",
        "\n",
        "    sampler = list(ParameterSampler(param_space, n_iter=N_TRIALS, random_state=SEED))\n",
        "\n",
        "    best_val_mae = float(\"inf\")\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "    best_it = None\n",
        "\n",
        "    pbar = tqdm(sampler, desc=f\"Tuning XGB ({cls})\", leave=False)\n",
        "    for params in pbar:\n",
        "        model = XGBRegressor(\n",
        "            objective=\"reg:squarederror\",\n",
        "            eval_metric=\"rmse\",       # put eval_metric in constructor (your build needs this)\n",
        "            n_estimators=N_ESTIMATORS,\n",
        "            tree_method=\"hist\",\n",
        "            n_jobs=N_JOBS,\n",
        "            random_state=SEED,\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        model = fit_with_early_stopping(model, X_tr, y_tr_log, X_val, y_val_log)\n",
        "\n",
        "        # compute MAE on ORIGINAL scale using best iteration\n",
        "        val_pred_log = predict_with_best_iteration(model, X_val)\n",
        "        val_pred = np.clip(np.expm1(val_pred_log), 0, None)\n",
        "        val_mae = float(mean_absolute_error(y_val, val_pred))\n",
        "\n",
        "        if val_mae < best_val_mae:\n",
        "            best_val_mae = val_mae\n",
        "            best_params = params\n",
        "            best_model = model\n",
        "            best_it = get_best_iteration(model)\n",
        "\n",
        "        pbar.set_postfix(best_val_MAE=f\"{best_val_mae:.1f}\", best_iter=best_it)\n",
        "\n",
        "    # Use the EARLY-STOPPED model directly for eval (no refit needed)\n",
        "    # This guarantees early stopping is actually used.\n",
        "    train_pred_log = predict_with_best_iteration(best_model, X_full)\n",
        "    test_pred_log  = predict_with_best_iteration(best_model, X_test)\n",
        "\n",
        "    pred_train = np.clip(np.expm1(train_pred_log), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(test_pred_log), 0, None)\n",
        "\n",
        "    mt = compute_metrics(y_full, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    best_models[cls] = {\"pre\": pre, \"model\": best_model, \"best_params\": best_params, \"best_iteration\": best_it}\n",
        "\n",
        "    results_rows.append({\n",
        "        \"class\": cls,\n",
        "        \"best_val_MAE\": best_val_mae,\n",
        "        \"best_iteration\": best_it,\n",
        "        \"best_params\": best_params,\n",
        "        \"n_train\": len(X_full_df),\n",
        "        \"n_test\": len(X_test_df),\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"train_MAE\":  mt[\"MAE\"],\n",
        "        \"train_R2\":   mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"],\n",
        "        \"test_MAE\":   ms[\"MAE\"],\n",
        "        \"test_R2\":    ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_rows).sort_values(\"class\")\n",
        "\n",
        "print(\"\\n===== FAST-TUNED XGBOOST RESULTS (early stopping truly applied) =====\")\n",
        "print(results_df.drop(columns=[\"best_params\"]).to_string(index=False))\n",
        "\n",
        "print(\"\\n===== BEST PARAMS PER CLASS =====\")\n",
        "for r in results_rows:\n",
        "    print(f\"\\nClass: {r['class']}\")\n",
        "    print(\"best_iteration:\", r[\"best_iteration\"])\n",
        "    print(r[\"best_params\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "67c21774a05e4542a1a7ac56ba94d5c7",
            "6d5a96aebfae48d5862bba53c6b06ebd",
            "bc2d832dc8964e4ca77bd430e46b8df2",
            "ec6f857e7a834d6d8bc7584695d3be5c",
            "cb67ac6a72324a238810b2294fb3fd35",
            "7277e52b937e4bfdbf6ea554c142d9b3",
            "9dd81e88797f4ad59cb48f56e70bb585",
            "a0bcc30d88b2433b84f4bd4c66bc5a5a",
            "83e0fd0f7d094756b406bfde29f8f676",
            "da42f48056b34725a4bb0f1e691a7e7b",
            "34db53feca414670b7b7628c2fbd603b",
            "0a5676106ad4435d917171931a7bb352",
            "6200da89bad043b4960aea9b63badf98",
            "2b109a4dbb6b4a33b85709ce7795bcaa",
            "06c93478791f422eb833d4d2ac094206",
            "ff3d9413c20f4f49aef93b5adb1e435a",
            "d09313eedc2e4af695a546b28023934e",
            "0062cebb711d4b6c8fe19e44a39f2f8e",
            "ecfda35ccdad42afbaaeca11f3b90638",
            "c030086d4bfa479b8ccd74d4d773a123",
            "8c86d110d68c4991886e2b8c5b066914",
            "e48a3dec592441829fbd40875925a5b9",
            "73bd8c4c00014ada8b62f5eb705204fc",
            "4ab0b10e39b9499a946244d43bc96dbb",
            "01ec3f426c8d46aeb70718f89fa656e6",
            "c7f6536cc9b24cf99cd30e0f75fd3bb0",
            "4a902efc073848ec9fbc402060e61f01",
            "c36cc0d755924aaaa7e86fd4025f202d",
            "964557ec968b47d9b4157181098bc3c1",
            "dc3145540a334a678149fbe6df01f9ce",
            "d48188b8815945e3a7397801c2708540",
            "74680444bf4e4ade86cb0a3a1117e654",
            "9905e9aa743047088fc8a64485377338"
          ]
        },
        "id": "gp4YO2CrD4Q8",
        "outputId": "31e7b503-302c-4aab-a12e-737bcfaf646e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67c21774a05e4542a1a7ac56ba94d5c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Classes:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a5676106ad4435d917171931a7bb352",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning XGB (native) (Business):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73bd8c4c00014ada8b62f5eb705204fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning XGB (native) (Economy):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== FAST-TUNED XGBOOST (native early stopping guaranteed) =====\n",
            "   class  best_val_MAE  best_iteration  n_train  n_test  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2\n",
            "Business   2485.001215            1161    24917    6229 2671.114769 1497.254891  0.957403 4524.993883 2613.511517 0.872749\n",
            " Economy    741.032731            2794    55083   13771  895.926243  444.355826  0.942456 1504.131592  748.190187 0.836884\n",
            "\n",
            "===== BEST PARAMS PER CLASS =====\n",
            "\n",
            "Class: Business\n",
            "best_iteration: 1161\n",
            "{'eta': 0.05, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 2.0, 'alpha': 0.0, 'gamma': 0.0}\n",
            "\n",
            "Class: Economy\n",
            "best_iteration: 2794\n",
            "{'eta': 0.05, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 2.0, 'alpha': 0.0, 'gamma': 0.0}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, ParameterSampler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import xgboost as xgb\n",
        "from scipy import sparse\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "N_TRIALS = 20\n",
        "NUM_BOOST_ROUND = 20000\n",
        "EARLY_STOPPING_ROUNDS = 200\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def add_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    # feature engineering (often improves accuracy more than tuning)\n",
        "    X = add_interactions(X)\n",
        "\n",
        "    y_log = np.log1p(y)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "\n",
        "def to_dmatrix(X, y=None):\n",
        "    # xgboost DMatrix likes CSR sparse for speed\n",
        "    if sparse.issparse(X):\n",
        "        X = X.tocsr()\n",
        "    return xgb.DMatrix(X, label=y)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# SAME split as your other models (reuse if already exists)\n",
        "# -------------------------------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Parameter space (fast + effective)\n",
        "# -------------------------------------------------\n",
        "param_space = {\n",
        "    \"eta\": [0.02, 0.03, 0.05, 0.08],           # learning_rate\n",
        "    \"max_depth\": [4, 6, 8, 10],\n",
        "    \"min_child_weight\": [1, 3, 5, 10],\n",
        "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"lambda\": [1.0, 2.0, 5.0, 10.0],           # reg_lambda\n",
        "    \"alpha\": [0.0, 0.1, 0.5, 1.0],             # reg_alpha\n",
        "    \"gamma\": [0.0, 0.05, 0.1, 0.2],\n",
        "}\n",
        "\n",
        "results_rows = []\n",
        "best_models = {}\n",
        "\n",
        "for cls in tqdm(sorted(df[\"class\"].dropna().unique()), desc=\"Classes\"):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_full_df, y_full, y_full_log = prepare_X_y(train_sub)\n",
        "    X_test_df, y_test, _          = prepare_X_y(test_sub)\n",
        "\n",
        "    # align columns\n",
        "    X_test_df = X_test_df.reindex(columns=X_full_df.columns)\n",
        "\n",
        "    # internal train/val split (fast tuning + early stopping)\n",
        "    X_tr_df, X_val_df, y_tr, y_val, y_tr_log, y_val_log = train_test_split(\n",
        "        X_full_df, y_full, y_full_log,\n",
        "        test_size=VAL_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # fit preprocessor on TRAIN only (no leakage)\n",
        "    pre = build_preprocessor(X_tr_df)\n",
        "    X_tr  = pre.fit_transform(X_tr_df)\n",
        "    X_val = pre.transform(X_val_df)\n",
        "\n",
        "    X_full = pre.transform(X_full_df)\n",
        "    X_test = pre.transform(X_test_df)\n",
        "\n",
        "    dtrain = to_dmatrix(X_tr, y_tr_log)\n",
        "    dval   = to_dmatrix(X_val, y_val_log)\n",
        "    dfull  = to_dmatrix(X_full, y_full_log)\n",
        "    dtest  = to_dmatrix(X_test, None)\n",
        "\n",
        "    sampler = list(ParameterSampler(param_space, n_iter=N_TRIALS, random_state=SEED))\n",
        "\n",
        "    best_val_mae = float(\"inf\")\n",
        "    best_params = None\n",
        "    best_booster = None\n",
        "    best_iter = None\n",
        "\n",
        "    pbar = tqdm(sampler, desc=f\"Tuning XGB (native) ({cls})\", leave=False)\n",
        "    for params in pbar:\n",
        "        base_params = {\n",
        "            \"objective\": \"reg:squarederror\",\n",
        "            \"eval_metric\": \"rmse\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"seed\": SEED,\n",
        "        }\n",
        "        base_params.update(params)\n",
        "\n",
        "        booster = xgb.train(\n",
        "            params=base_params,\n",
        "            dtrain=dtrain,\n",
        "            num_boost_round=NUM_BOOST_ROUND,\n",
        "            evals=[(dval, \"val\")],\n",
        "            early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "\n",
        "        # early stopping DEFINITELY applied here:\n",
        "        it = int(booster.best_iteration)\n",
        "\n",
        "        # validate on ORIGINAL scale\n",
        "        val_pred_log = booster.predict(dval, iteration_range=(0, it + 1))\n",
        "        val_pred = np.clip(np.expm1(val_pred_log), 0, None)\n",
        "        val_mae = float(mean_absolute_error(y_val, val_pred))\n",
        "\n",
        "        if val_mae < best_val_mae:\n",
        "            best_val_mae = val_mae\n",
        "            best_params = base_params\n",
        "            best_booster = booster\n",
        "            best_iter = it\n",
        "\n",
        "        pbar.set_postfix(best_val_MAE=f\"{best_val_mae:.1f}\", best_iter=best_iter)\n",
        "\n",
        "    # Evaluate using the best booster + best_iter (true early stopping)\n",
        "    train_pred_log = best_booster.predict(dfull, iteration_range=(0, best_iter + 1))\n",
        "    test_pred_log  = best_booster.predict(dtest, iteration_range=(0, best_iter + 1))\n",
        "\n",
        "    pred_train = np.clip(np.expm1(train_pred_log), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(test_pred_log), 0, None)\n",
        "\n",
        "    mt = compute_metrics(y_full, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    best_models[cls] = {\"pre\": pre, \"booster\": best_booster, \"best_iter\": best_iter, \"params\": best_params}\n",
        "\n",
        "    results_rows.append({\n",
        "        \"class\": cls,\n",
        "        \"best_val_MAE\": best_val_mae,\n",
        "        \"best_iteration\": best_iter,\n",
        "        \"n_train\": len(X_full_df),\n",
        "        \"n_test\": len(X_test_df),\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"train_MAE\":  mt[\"MAE\"],\n",
        "        \"train_R2\":   mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"],\n",
        "        \"test_MAE\":   ms[\"MAE\"],\n",
        "        \"test_R2\":    ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_rows).sort_values(\"class\")\n",
        "\n",
        "print(\"\\n===== FAST-TUNED XGBOOST (native early stopping guaranteed) =====\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n===== BEST PARAMS PER CLASS =====\")\n",
        "for cls in best_models:\n",
        "    print(f\"\\nClass: {cls}\")\n",
        "    print(\"best_iteration:\", best_models[cls][\"best_iter\"])\n",
        "    # show params without huge print\n",
        "    p = dict(best_models[cls][\"params\"])\n",
        "    print({k: p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\",\"lambda\",\"alpha\",\"gamma\"]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "4d22c341676d4b72b7d82202765832b3",
            "1f80d45bf77c493da4277be16e771289",
            "6525ec11eabb490da6bdbe4957d26065",
            "6a9185b2467d4ccdac846629ea582034",
            "d552d165be3944ebbc8beab1c653f39e",
            "233cdfbe506e4e5ea658f86ca2219ffc",
            "b5e3d20d54074b15bceb5fa19e209a95",
            "0e3cdc977e4c4940be769e2bb4c9e908",
            "b5ca34d95d0844ed8ee5477da283c1db",
            "d64bb4905a0e41aabe43aae724ddf5e6",
            "19bbeaa3d92747e2876780c29e20ec11",
            "ebb1ca6a56c34071a2a0f4bd0390b38e",
            "2a8cb9b9a4a84b8291a380b7bee6e3fc",
            "74cb9bfbeade4db39b1c6ddd607d6f8e",
            "2e6009018870423d8569b343fb4e0822",
            "1abcb9b4b35a4398a078dba670be0509",
            "b331ab6e2dc24237908e08b45fc2684d",
            "22b2eb7c1d834b9784be238165099636",
            "ca045ac421b54d9093ddb33d9c905c45",
            "cbf4e243d7b44e418f8e8224305ca310",
            "f6193eb1b55e4fd0bbe0990a29358799",
            "39748b2ebd764d0fb68cc785f9799388",
            "574e84da0aa64a75bb7c65c35fe89a20",
            "73f0942d0def451bb011a89f29aab72c",
            "3dd1957d58b54533a6f615c1a3770cad",
            "ea743b3fecca416297eb5dc0a0d058d8",
            "357d11b40bb64f8cb2f45330280a8275",
            "6a4083e046d64438a9104294a1edd1c9",
            "e076f7a87c224ba292832b436e9b2511",
            "eeedfe8c274947528b673cb31b7af172",
            "5c668a4338774fd4b08b131ae419d870",
            "f1ebb83f071e497fb28051f19be23326",
            "c28d82dc40fe4ff1b0452c9d302776c5"
          ]
        },
        "id": "CfHCRa-izbmP",
        "outputId": "6436f853-04f9-44d5-9ce8-0476fc1772f0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d22c341676d4b72b7d82202765832b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Classes:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebb1ca6a56c34071a2a0f4bd0390b38e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training CatBoost (Business):   0%|          | 0/8000 [00:00<?, ?iter/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "574e84da0aa64a75bb7c65c35fe89a20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training CatBoost (Economy):   0%|          | 0/8000 [00:00<?, ?iter/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== CATBOOST RESULTS (separate model per class | same split indices) =====\n",
            "   class  n_train  n_test  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2\n",
            "Business    24917    6229 3725.270302 2271.370348  0.917146 4566.731373 2775.129298 0.870390\n",
            " Economy    55083   13771 1295.171633  708.008982  0.879743 1570.297649  832.612419 0.822218\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# CATBOOST (separate model per class) + tqdm progress bars\n",
        "# - SAME train/test split via train_idx/test_idx (fair comparison)\n",
        "# - Drop flight\n",
        "# - log1p target, expm1 back for metrics\n",
        "# - Adds route + route_airline features (often boosts accuracy)\n",
        "# - Progress bar by training in chunks (init_model)\n",
        "# Assumes df is already loaded\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "TOTAL_ITERS = 8000   # total trees\n",
        "STEP_ITERS  = 250    # chunk size for progress bar\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def add_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def prepare_class_df(df_sub: pd.DataFrame):\n",
        "    \"\"\"Returns X, y (price), y_log, cat_cols for CatBoost.\"\"\"\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    # Add engineered features\n",
        "    X = add_interactions(X)\n",
        "\n",
        "    # Detect categoricals for CatBoost\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "\n",
        "    # Fill categoricals\n",
        "    for c in cat_cols:\n",
        "        X[c] = X[c].astype(\"string\").fillna(\"missing\").astype(str)\n",
        "\n",
        "    # Convert numeric + impute\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "    for c in num_cols:\n",
        "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "    if len(num_cols) > 0:\n",
        "        med = X[num_cols].median(numeric_only=True)\n",
        "        X[num_cols] = X[num_cols].fillna(med)\n",
        "\n",
        "    return X, y, y_log, cat_cols\n",
        "\n",
        "def fit_catboost_with_progress(train_pool: Pool, cls_name: str,\n",
        "                               total_iters: int = TOTAL_ITERS, step_iters: int = STEP_ITERS):\n",
        "    \"\"\"\n",
        "    Train CatBoost in chunks to show a tqdm progress bar.\n",
        "    \"\"\"\n",
        "    init_model = None\n",
        "    done = 0\n",
        "\n",
        "    # Reasonable \"strong but stable\" defaults (not heavy tuning)\n",
        "    base_params = dict(\n",
        "        loss_function=\"RMSE\",\n",
        "        random_seed=SEED,\n",
        "        learning_rate=0.05,\n",
        "        depth=8,\n",
        "        l2_leaf_reg=10,\n",
        "        bagging_temperature=0.5,\n",
        "        random_strength=1.0,\n",
        "        rsm=0.9,\n",
        "        verbose=False,\n",
        "        allow_writing_files=False,\n",
        "        thread_count=-1\n",
        "    )\n",
        "\n",
        "    pbar = tqdm(total=total_iters, desc=f\"Training CatBoost ({cls_name})\", unit=\"iter\")\n",
        "    while done < total_iters:\n",
        "        iters = min(step_iters, total_iters - done)\n",
        "        m = CatBoostRegressor(iterations=iters, **base_params)\n",
        "        m.fit(train_pool, init_model=init_model)\n",
        "        init_model = m\n",
        "        done += iters\n",
        "        pbar.update(iters)\n",
        "    pbar.close()\n",
        "\n",
        "    return init_model\n",
        "\n",
        "# -------------------------------------------------\n",
        "# SAME split as your other models\n",
        "# -------------------------------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Train separate CatBoost models per class\n",
        "# -------------------------------------------------\n",
        "rows = []\n",
        "models = {}\n",
        "\n",
        "for cls in tqdm(sorted(df[\"class\"].dropna().unique()), desc=\"Classes\"):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_train, y_train, y_train_log, cat_cols = prepare_class_df(train_sub)\n",
        "    X_test,  y_test,  _,          _        = prepare_class_df(test_sub)\n",
        "\n",
        "    # Align columns\n",
        "    X_test = X_test.reindex(columns=X_train.columns)\n",
        "\n",
        "    train_pool = Pool(X_train, y_train_log, cat_features=cat_cols)\n",
        "    test_pool  = Pool(X_test,  cat_features=cat_cols)\n",
        "\n",
        "    model = fit_catboost_with_progress(train_pool, cls_name=str(cls))\n",
        "    models[cls] = model\n",
        "\n",
        "    # Predict -> back to original scale\n",
        "    pred_train = np.clip(np.expm1(model.predict(train_pool)), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(model.predict(test_pool)),  0, None)\n",
        "\n",
        "    mt = compute_metrics(y_train, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    rows.append({\n",
        "        \"class\": cls,\n",
        "        \"n_train\": len(X_train),\n",
        "        \"n_test\": len(X_test),\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"train_MAE\":  mt[\"MAE\"],\n",
        "        \"train_R2\":   mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"],\n",
        "        \"test_MAE\":   ms[\"MAE\"],\n",
        "        \"test_R2\":    ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows).sort_values(\"class\")\n",
        "\n",
        "print(\"\\n===== CATBOOST RESULTS (separate model per class | same split indices) =====\")\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bc849f6f610d428989260f9d82188402",
            "4eb4aa71adad445c8c0b753a9f12c65f",
            "fc5caac128ee42648d888faeeea7f372",
            "cd2d0d5188e441f99ef53aaeb6036000",
            "1d9ce790e9d9446ca4fb00afa70a9162",
            "ad3bc5a149774b0ea8c6f6bdd545dda7",
            "ef31d2227eb44145b38484e839648245",
            "60f0622d53d3418799deb7939ad718c1",
            "85f05d356fde4f62a86f3ac6877f2fa7",
            "7e7d531009c04a0d80e828c64f875d04",
            "99a367688c3c4c7aae61a3bbd8088e49",
            "cb70c53c32c445e79fcd7ad9979d2ad2",
            "4f947d670e7643e8ab2778cd038f1862",
            "3aab198651eb40df99032a4eb2351b87",
            "095f2fb383e3491684fb4127b90106ef",
            "5b4dd143a5b34e9cb014c28b5a5f48a6",
            "8eaba66192ca4f97a8403147f291228d",
            "cc7da388345443359aaee391fbefe4d1",
            "c9b8a978055b4f9f87c4acc6fc1a0f0a",
            "2b567baa638446da806c5963de06e394",
            "84851c6eefcc4464a159aad8b17f7ef8",
            "5130f4596bee40d9baff16365063e769",
            "64ee4173c6ab4588a8f3a7968a8958d8",
            "8aac9a6dba924a32a781f5227db32f01",
            "222b006e5e294ed0b74a7ec6e1de8e5d",
            "77a44da635394a54b4757aec99dd0d6b",
            "8bafcf7568d245049c78041ad69f200d",
            "7eb513e865d34004b64fa7abca990f5b",
            "720e26ffa95b43e2adfec319fa391cd2",
            "dc854aa18a4e455aa9772399ef4ac919",
            "1ae1409164a149d88d7071d37b9c9ac9",
            "d33188bd148e43a594a7c41a16db9bbe",
            "33ad9dfa0d1d426b862b4b8bdfe5634f"
          ]
        },
        "id": "AEI5p397SdBj",
        "outputId": "71b24477-5027-409f-c478-712d9a454508"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc849f6f610d428989260f9d82188402",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Classes:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb70c53c32c445e79fcd7ad9979d2ad2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning CatBoost FAST (Business):   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.2677182\ttest: 0.2662753\tbest: 0.2662753 (0)\ttotal: 22.4ms\tremaining: 44.7s\n",
            "200:\tlearn: 0.1204399\ttest: 0.1198070\tbest: 0.1198070 (200)\ttotal: 23.2s\tremaining: 3m 27s\n",
            "400:\tlearn: 0.1132230\ttest: 0.1139177\tbest: 0.1139177 (400)\ttotal: 46.7s\tremaining: 3m 6s\n",
            "600:\tlearn: 0.1089742\ttest: 0.1106767\tbest: 0.1106763 (596)\ttotal: 1m 7s\tremaining: 2m 35s\n",
            "800:\tlearn: 0.1047074\ttest: 0.1074610\tbest: 0.1074610 (800)\ttotal: 1m 29s\tremaining: 2m 14s\n",
            "1000:\tlearn: 0.1027851\ttest: 0.1061832\tbest: 0.1061824 (995)\ttotal: 1m 53s\tremaining: 1m 53s\n",
            "1200:\tlearn: 0.1005082\ttest: 0.1045897\tbest: 0.1045897 (1200)\ttotal: 2m 16s\tremaining: 1m 30s\n",
            "1400:\tlearn: 0.0983711\ttest: 0.1031872\tbest: 0.1031872 (1399)\ttotal: 2m 39s\tremaining: 1m 8s\n",
            "1600:\tlearn: 0.0965909\ttest: 0.1018271\tbest: 0.1018271 (1600)\ttotal: 3m 3s\tremaining: 45.8s\n",
            "1800:\tlearn: 0.0951737\ttest: 0.1007474\tbest: 0.1007474 (1800)\ttotal: 3m 28s\tremaining: 23s\n",
            "1999:\tlearn: 0.0934565\ttest: 0.0996506\tbest: 0.0996492 (1998)\ttotal: 3m 54s\tremaining: 0us\n",
            "bestTest = 0.09964923111\n",
            "bestIteration = 1998\n",
            "Shrink model to first 1999 iterations.\n",
            "0:\tlearn: 0.2649497\ttest: 0.2634224\tbest: 0.2634224 (0)\ttotal: 121ms\tremaining: 4m 2s\n",
            "200:\tlearn: 0.1184686\ttest: 0.1181064\tbest: 0.1181064 (199)\ttotal: 20.3s\tremaining: 3m 1s\n",
            "400:\tlearn: 0.1113842\ttest: 0.1124634\tbest: 0.1124634 (400)\ttotal: 41.5s\tremaining: 2m 45s\n",
            "600:\tlearn: 0.1104424\ttest: 0.1117853\tbest: 0.1117853 (590)\ttotal: 56.5s\tremaining: 2m 11s\n",
            "800:\tlearn: 0.1082902\ttest: 0.1102162\tbest: 0.1102161 (796)\ttotal: 1m 18s\tremaining: 1m 57s\n",
            "1000:\tlearn: 0.1067578\ttest: 0.1091625\tbest: 0.1091622 (945)\ttotal: 1m 41s\tremaining: 1m 40s\n",
            "1200:\tlearn: 0.1048456\ttest: 0.1076270\tbest: 0.1076270 (1200)\ttotal: 2m 3s\tremaining: 1m 21s\n",
            "1400:\tlearn: 0.1026267\ttest: 0.1057380\tbest: 0.1057380 (1400)\ttotal: 2m 22s\tremaining: 1m\n",
            "1600:\tlearn: 0.1012633\ttest: 0.1048199\tbest: 0.1048199 (1600)\ttotal: 2m 43s\tremaining: 40.7s\n",
            "1800:\tlearn: 0.0982799\ttest: 0.1026366\tbest: 0.1026332 (1788)\ttotal: 3m 7s\tremaining: 20.7s\n",
            "1999:\tlearn: 0.0981045\ttest: 0.1025488\tbest: 0.1025488 (1999)\ttotal: 3m 29s\tremaining: 0us\n",
            "bestTest = 0.1025487981\n",
            "bestIteration = 1999\n",
            "0:\tlearn: 0.2721696\ttest: 0.2707388\tbest: 0.2707388 (0)\ttotal: 17.1ms\tremaining: 34.2s\n",
            "200:\tlearn: 0.1266183\ttest: 0.1247603\tbest: 0.1247603 (200)\ttotal: 11.5s\tremaining: 1m 43s\n",
            "400:\tlearn: 0.1211933\ttest: 0.1201631\tbest: 0.1201631 (400)\ttotal: 22.2s\tremaining: 1m 28s\n",
            "600:\tlearn: 0.1169879\ttest: 0.1168818\tbest: 0.1168818 (600)\ttotal: 30.7s\tremaining: 1m 11s\n",
            "800:\tlearn: 0.1145461\ttest: 0.1149086\tbest: 0.1149086 (800)\ttotal: 40.1s\tremaining: 1m\n",
            "1000:\tlearn: 0.1126468\ttest: 0.1134316\tbest: 0.1134316 (1000)\ttotal: 50.2s\tremaining: 50.1s\n",
            "1200:\tlearn: 0.1106773\ttest: 0.1119970\tbest: 0.1119970 (1200)\ttotal: 1m 1s\tremaining: 40.9s\n",
            "1400:\tlearn: 0.1089031\ttest: 0.1107112\tbest: 0.1107086 (1394)\ttotal: 1m 10s\tremaining: 30.3s\n",
            "1600:\tlearn: 0.1081818\ttest: 0.1101636\tbest: 0.1101636 (1600)\ttotal: 1m 21s\tremaining: 20.2s\n",
            "1800:\tlearn: 0.1069020\ttest: 0.1092141\tbest: 0.1092141 (1799)\ttotal: 1m 32s\tremaining: 10.2s\n",
            "1999:\tlearn: 0.1059602\ttest: 0.1084902\tbest: 0.1084901 (1992)\ttotal: 1m 43s\tremaining: 0us\n",
            "bestTest = 0.1084900984\n",
            "bestIteration = 1992\n",
            "Shrink model to first 1993 iterations.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64ee4173c6ab4588a8f3a7968a8958d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tuning CatBoost FAST (Economy):   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.5005674\ttest: 0.4946968\tbest: 0.4946968 (0)\ttotal: 268ms\tremaining: 8m 56s\n",
            "200:\tlearn: 0.2175339\ttest: 0.2153819\tbest: 0.2153819 (200)\ttotal: 30.1s\tremaining: 4m 29s\n",
            "400:\tlearn: 0.2085612\ttest: 0.2089334\tbest: 0.2089332 (398)\ttotal: 56.9s\tremaining: 3m 46s\n",
            "600:\tlearn: 0.2018535\ttest: 0.2045792\tbest: 0.2045792 (600)\ttotal: 1m 23s\tremaining: 3m 14s\n",
            "800:\tlearn: 0.1980525\ttest: 0.2022132\tbest: 0.2022132 (800)\ttotal: 1m 51s\tremaining: 2m 46s\n",
            "1000:\tlearn: 0.1959541\ttest: 0.2011822\tbest: 0.2011817 (996)\ttotal: 2m 20s\tremaining: 2m 19s\n",
            "1200:\tlearn: 0.1936771\ttest: 0.1999674\tbest: 0.1999674 (1200)\ttotal: 2m 48s\tremaining: 1m 52s\n",
            "1400:\tlearn: 0.1909353\ttest: 0.1984926\tbest: 0.1984923 (1399)\ttotal: 3m 18s\tremaining: 1m 24s\n",
            "1600:\tlearn: 0.1889878\ttest: 0.1975081\tbest: 0.1975077 (1599)\ttotal: 3m 47s\tremaining: 56.7s\n",
            "1800:\tlearn: 0.1862982\ttest: 0.1960461\tbest: 0.1960461 (1800)\ttotal: 4m 16s\tremaining: 28.4s\n",
            "1999:\tlearn: 0.1845109\ttest: 0.1952216\tbest: 0.1952216 (1999)\ttotal: 4m 45s\tremaining: 0us\n",
            "bestTest = 0.1952216088\n",
            "bestIteration = 1999\n",
            "0:\tlearn: 0.4998669\ttest: 0.4939494\tbest: 0.4939494 (0)\ttotal: 146ms\tremaining: 4m 52s\n",
            "200:\tlearn: 0.2157815\ttest: 0.2145737\tbest: 0.2145737 (200)\ttotal: 28s\tremaining: 4m 10s\n",
            "400:\tlearn: 0.2087624\ttest: 0.2098873\tbest: 0.2098873 (400)\ttotal: 55.9s\tremaining: 3m 42s\n",
            "600:\tlearn: 0.2072284\ttest: 0.2089015\tbest: 0.2089010 (595)\ttotal: 1m 22s\tremaining: 3m 11s\n",
            "800:\tlearn: 0.2035901\ttest: 0.2065958\tbest: 0.2065867 (745)\ttotal: 1m 48s\tremaining: 2m 42s\n",
            "1000:\tlearn: 0.2019551\ttest: 0.2055478\tbest: 0.2055469 (949)\ttotal: 2m 16s\tremaining: 2m 16s\n",
            "bestTest = 0.2055468546\n",
            "bestIteration = 949\n",
            "Shrink model to first 950 iterations.\n",
            "0:\tlearn: 0.5109399\ttest: 0.5052693\tbest: 0.5052693 (0)\ttotal: 60.8ms\tremaining: 2m 1s\n",
            "200:\tlearn: 0.2290530\ttest: 0.2241401\tbest: 0.2241401 (200)\ttotal: 13.4s\tremaining: 1m 59s\n",
            "400:\tlearn: 0.2203078\ttest: 0.2172837\tbest: 0.2172837 (400)\ttotal: 25.4s\tremaining: 1m 41s\n",
            "600:\tlearn: 0.2142267\ttest: 0.2129406\tbest: 0.2129402 (597)\ttotal: 37.8s\tremaining: 1m 27s\n",
            "800:\tlearn: 0.2111845\ttest: 0.2109705\tbest: 0.2109705 (800)\ttotal: 50.2s\tremaining: 1m 15s\n",
            "1000:\tlearn: 0.2076225\ttest: 0.2085343\tbest: 0.2085343 (1000)\ttotal: 1m 3s\tremaining: 1m 2s\n",
            "1200:\tlearn: 0.2059915\ttest: 0.2076108\tbest: 0.2076108 (1200)\ttotal: 1m 15s\tremaining: 50.2s\n",
            "1400:\tlearn: 0.2044040\ttest: 0.2065509\tbest: 0.2065509 (1400)\ttotal: 1m 28s\tremaining: 37.7s\n",
            "1600:\tlearn: 0.2023994\ttest: 0.2053048\tbest: 0.2053048 (1600)\ttotal: 1m 40s\tremaining: 25.1s\n",
            "1800:\tlearn: 0.2007724\ttest: 0.2042363\tbest: 0.2042363 (1800)\ttotal: 1m 53s\tremaining: 12.5s\n",
            "1999:\tlearn: 0.1994363\ttest: 0.2034502\tbest: 0.2034502 (1999)\ttotal: 2m 5s\tremaining: 0us\n",
            "bestTest = 0.2034501575\n",
            "bestIteration = 1999\n",
            "\n",
            "===== FAST TUNED CATBOOST RESULTS (early stopping) =====\n",
            "   class  used_gpu  best_val_MAE  best_iter  n_train  n_test  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2\n",
            "Business      True   3601.167049       1998    24917    6229 5312.076920 3497.971604  0.831529 5492.155407 3668.439006 0.812539\n",
            " Economy      True    926.483296       1999    55083   13771 1575.895614  887.466734  0.821962 1673.831311  929.123949 0.798002\n",
            "\n",
            "===== BEST PARAMS PER CLASS =====\n",
            "\n",
            "Class: Business | used_gpu=True\n",
            "best_iter: 1998\n",
            "\n",
            "Class: Economy | used_gpu=True\n",
            "best_iter: 1999\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split, ParameterSampler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "# (Optional) hide only this noisy warning type\n",
        "warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "# FAST settings\n",
        "N_TRIALS =3           # very fast (increase to 10-20 later)\n",
        "MAX_ITERS = 2000       # smaller max trees\n",
        "OD_WAIT = 150           # smaller patience -> faster early stop\n",
        "USE_GPU = True          # try GPU; will fall back to CPU if unavailable\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx, test_size=TEST_SIZE, random_state=SEED, shuffle=True, stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def add_features(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    if \"departure_time\" in X.columns and \"arrival_time\" in X.columns:\n",
        "        X[\"dep_arr_pair\"] = X[\"departure_time\"].astype(str) + \"_\" + X[\"arrival_time\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def robust_numeric_impute(X: pd.DataFrame, num_cols):\n",
        "    \"\"\"\n",
        "    Fill numeric columns safely:\n",
        "    - if column has some numbers -> fill NaNs with median\n",
        "    - if column is all NaN -> fill with 0 (prevents empty-slice warnings)\n",
        "    \"\"\"\n",
        "    if not num_cols:\n",
        "        return X\n",
        "\n",
        "    for c in num_cols:\n",
        "        col = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "        if col.notna().any():\n",
        "            med = float(col.median())\n",
        "            X[c] = col.fillna(med)\n",
        "        else:\n",
        "            X[c] = 0.0\n",
        "    return X\n",
        "\n",
        "def prepare_class_df(df_sub: pd.DataFrame):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    X = add_features(X)\n",
        "\n",
        "    # categorical columns for CatBoost\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    for c in cat_cols:\n",
        "        X[c] = X[c].astype(\"string\").fillna(\"missing\").astype(str)\n",
        "\n",
        "    # treat the rest as numeric\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "    X = robust_numeric_impute(X, num_cols)\n",
        "\n",
        "    return X, y, y_log, cat_cols\n",
        "\n",
        "# ---- SAME split as other models ----\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# Small but meaningful search space (fast)\n",
        "param_space = {\n",
        "    \"depth\": [6, 8, 10],\n",
        "    \"learning_rate\": [0.05, 0.08],\n",
        "    \"l2_leaf_reg\": [5, 10, 20],\n",
        "    \"bagging_temperature\": [0.0, 0.5],\n",
        "}\n",
        "\n",
        "rows = []\n",
        "best_models = {}\n",
        "\n",
        "for cls in tqdm(sorted(df[\"class\"].dropna().unique()), desc=\"Classes\"):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_full, y_full, y_full_log, cat_cols = prepare_class_df(train_sub)\n",
        "    X_test, y_test, _, _                 = prepare_class_df(test_sub)\n",
        "    X_test = X_test.reindex(columns=X_full.columns)\n",
        "\n",
        "    X_tr, X_val, y_tr, y_val, y_tr_log, y_val_log = train_test_split(\n",
        "        X_full, y_full, y_full_log, test_size=VAL_SIZE, random_state=SEED, shuffle=True\n",
        "    )\n",
        "\n",
        "    train_pool = Pool(X_tr, y_tr_log, cat_features=cat_cols)\n",
        "    val_pool   = Pool(X_val, y_val_log, cat_features=cat_cols)\n",
        "\n",
        "    sampler = list(ParameterSampler(param_space, n_iter=N_TRIALS, random_state=SEED))\n",
        "\n",
        "    best_val_mae = float(\"inf\")\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "    used_gpu = False\n",
        "\n",
        "    for params in tqdm(sampler, desc=f\"Tuning CatBoost FAST ({cls})\", leave=False):\n",
        "        base_kwargs = dict(\n",
        "            loss_function=\"RMSE\",\n",
        "            iterations=MAX_ITERS,\n",
        "            od_type=\"Iter\",\n",
        "            od_wait=OD_WAIT,\n",
        "            use_best_model=True,\n",
        "            random_seed=SEED,\n",
        "            allow_writing_files=False,\n",
        "            verbose=200  # print something so it doesn't look stuck\n",
        "        )\n",
        "\n",
        "        model = None\n",
        "\n",
        "        # Try GPU first (fastest). If not available, fall back to CPU.\n",
        "        if USE_GPU:\n",
        "            try:\n",
        "                model = CatBoostRegressor(\n",
        "                    **base_kwargs,\n",
        "                    task_type=\"GPU\",\n",
        "                    devices=\"0\",\n",
        "                    **params\n",
        "                )\n",
        "                model.fit(train_pool, eval_set=val_pool)\n",
        "                used_gpu = True\n",
        "            except Exception:\n",
        "                model = None\n",
        "\n",
        "        if model is None:\n",
        "            model = CatBoostRegressor(\n",
        "                **base_kwargs,\n",
        "                thread_count=-1,\n",
        "                **params\n",
        "            )\n",
        "            model.fit(train_pool, eval_set=val_pool)\n",
        "\n",
        "        val_pred = np.clip(np.expm1(model.predict(val_pool)), 0, None)\n",
        "        val_mae = float(mean_absolute_error(y_val, val_pred))\n",
        "\n",
        "        if val_mae < best_val_mae:\n",
        "            best_val_mae = val_mae\n",
        "            best_model = model\n",
        "            best_params = params\n",
        "\n",
        "    # Evaluate best on full train/test\n",
        "    full_pool = Pool(X_full, y_full_log, cat_features=cat_cols)\n",
        "    test_pool = Pool(X_test, cat_features=cat_cols)\n",
        "\n",
        "    pred_train = np.clip(np.expm1(best_model.predict(full_pool)), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(best_model.predict(test_pool)), 0, None)\n",
        "\n",
        "    mt = compute_metrics(y_full, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    best_models[cls] = best_model\n",
        "\n",
        "    rows.append({\n",
        "        \"class\": cls,\n",
        "        \"used_gpu\": used_gpu,\n",
        "        \"best_val_MAE\": best_val_mae,\n",
        "        \"best_iter\": best_model.get_best_iteration(),\n",
        "        \"n_train\": len(X_full),\n",
        "        \"n_test\": len(X_test),\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"train_MAE\":  mt[\"MAE\"],\n",
        "        \"train_R2\":   mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"],\n",
        "        \"test_MAE\":   ms[\"MAE\"],\n",
        "        \"test_R2\":    ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows).sort_values(\"class\")\n",
        "\n",
        "print(\"\\n===== FAST TUNED CATBOOST RESULTS (early stopping) =====\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n===== BEST PARAMS PER CLASS =====\")\n",
        "for cls, model in best_models.items():\n",
        "    print(f\"\\nClass: {cls} | used_gpu={results_df.loc[results_df['class']==cls,'used_gpu'].values[0]}\")\n",
        "    print(\"best_iter:\", model.get_best_iteration())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzSVKZk-WZ15",
        "outputId": "46223249-5f73-40d2-cba6-f83050268385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Business -> RandomForest ...\n",
            "\n",
            "Training Economy -> XGBoost (early stopping) ...\n",
            "[0]\tval-rmse:0.50349\n",
            "[200]\tval-rmse:0.19345\n",
            "[400]\tval-rmse:0.18339\n",
            "[600]\tval-rmse:0.17689\n",
            "[800]\tval-rmse:0.17348\n",
            "[1000]\tval-rmse:0.17113\n",
            "[1200]\tval-rmse:0.16962\n",
            "[1400]\tval-rmse:0.16868\n",
            "[1600]\tval-rmse:0.16774\n",
            "[1800]\tval-rmse:0.16732\n",
            "[2000]\tval-rmse:0.16685\n",
            "[2200]\tval-rmse:0.16661\n",
            "[2400]\tval-rmse:0.16631\n",
            "[2600]\tval-rmse:0.16618\n",
            "[2800]\tval-rmse:0.16606\n",
            "[2994]\tval-rmse:0.16614\n",
            "\n",
            "===== HYBRID MODEL RESULTS =====\n",
            "   class           model  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2  best_iteration\n",
            "Business    RandomForest 2674.226576 1361.951775  0.957303 4498.154855 2362.310699 0.874254             NaN\n",
            " Economy XGBoost(native)  895.926243  444.355826  0.942456 1504.131592  748.190187 0.836884          2794.0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# HYBRID BEST MODEL BY CLASS\n",
        "#   Business -> Random Forest (tuned params from your results)\n",
        "#   Economy  -> XGBoost (native train + early stopping, best params)\n",
        "#\n",
        "# - SAME train/test split via train_idx/test_idx (fair)\n",
        "# - Drop flight\n",
        "# - log1p target transform, expm1 back to price for metrics\n",
        "# - Uses route + route_airline features (as in your XGBoost run)\n",
        "# - Includes progress bars\n",
        "#\n",
        "# Assumes df is already loaded\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import xgboost as xgb\n",
        "from scipy import sparse\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "# --- Best params you reported ---\n",
        "RF_PARAMS = {\n",
        "    \"min_samples_split\": 5,\n",
        "    \"min_samples_leaf\": 1,\n",
        "    \"max_features\": 0.8,\n",
        "    \"max_depth\": None,\n",
        "}\n",
        "\n",
        "XGB_PARAMS = {\n",
        "    \"eta\": 0.05,\n",
        "    \"max_depth\": 10,\n",
        "    \"min_child_weight\": 5,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"lambda\": 2.0,\n",
        "    \"alpha\": 0.0,\n",
        "    \"gamma\": 0.0,\n",
        "}\n",
        "\n",
        "XGB_NUM_BOOST_ROUND = 20000\n",
        "XGB_EARLY_STOPPING_ROUNDS = 200\n",
        "XGB_TREE_METHOD = \"hist\"\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "\n",
        "def add_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    X = add_interactions(X)\n",
        "    return X, y, y_log\n",
        "\n",
        "\n",
        "def get_feature_lists(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "    return num_cols, cat_cols\n",
        "\n",
        "\n",
        "def make_onehot_preprocessor(num_cols, cat_cols):\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "\n",
        "\n",
        "def to_csr(X):\n",
        "    if sparse.issparse(X):\n",
        "        return X.tocsr()\n",
        "    return X\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) Same split\n",
        "# ============================================================\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# Hold trained models here\n",
        "hybrid_models = {}\n",
        "results = []\n",
        "\n",
        "# ============================================================\n",
        "# 2) BUSINESS -> Random Forest\n",
        "# ============================================================\n",
        "cls = \"Business\"\n",
        "train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "X_train, y_train, y_train_log = prepare_X_y(train_sub)\n",
        "X_test,  y_test,  _           = prepare_X_y(test_sub)\n",
        "X_test = X_test.reindex(columns=X_train.columns)\n",
        "\n",
        "num_cols, cat_cols = get_feature_lists(X_train)\n",
        "pre = make_onehot_preprocessor(num_cols, cat_cols)\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=800,       # strong default; matches your tuning style\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    bootstrap=True,\n",
        "    **RF_PARAMS\n",
        ")\n",
        "\n",
        "rf_pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", rf)])\n",
        "\n",
        "print(\"\\nTraining Business -> RandomForest ...\")\n",
        "rf_pipe.fit(X_train, y_train_log)\n",
        "\n",
        "pred_train = np.clip(np.expm1(rf_pipe.predict(X_train)), 0, None)\n",
        "pred_test  = np.clip(np.expm1(rf_pipe.predict(X_test)),  0, None)\n",
        "\n",
        "mt = compute_metrics(y_train, pred_train)\n",
        "ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "hybrid_models[\"Business\"] = {\"type\": \"rf\", \"model\": rf_pipe, \"features\": X_train.columns.tolist()}\n",
        "\n",
        "results.append({\n",
        "    \"class\": \"Business\",\n",
        "    \"model\": \"RandomForest\",\n",
        "    \"train_RMSE\": mt[\"RMSE\"], \"train_MAE\": mt[\"MAE\"], \"train_R2\": mt[\"R2\"],\n",
        "    \"test_RMSE\":  ms[\"RMSE\"], \"test_MAE\":  ms[\"MAE\"], \"test_R2\":  ms[\"R2\"],\n",
        "})\n",
        "\n",
        "# ============================================================\n",
        "# 3) ECONOMY -> XGBoost (native + early stopping)\n",
        "# ============================================================\n",
        "cls = \"Economy\"\n",
        "train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "X_full, y_full, y_full_log = prepare_X_y(train_sub)\n",
        "X_test_df, y_test, _       = prepare_X_y(test_sub)\n",
        "X_test_df = X_test_df.reindex(columns=X_full.columns)\n",
        "\n",
        "# internal train/val split for early stopping\n",
        "X_tr_df, X_val_df, y_tr, y_val, y_tr_log, y_val_log = train_test_split(\n",
        "    X_full, y_full, y_full_log,\n",
        "    test_size=VAL_SIZE,\n",
        "    random_state=SEED,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "num_cols, cat_cols = get_feature_lists(X_tr_df)\n",
        "pre = make_onehot_preprocessor(num_cols, cat_cols)\n",
        "\n",
        "X_tr  = to_csr(pre.fit_transform(X_tr_df))\n",
        "X_val = to_csr(pre.transform(X_val_df))\n",
        "X_full_mat = to_csr(pre.transform(X_full))\n",
        "X_test_mat = to_csr(pre.transform(X_test_df))\n",
        "\n",
        "dtrain = xgb.DMatrix(X_tr, label=y_tr_log)\n",
        "dval   = xgb.DMatrix(X_val, label=y_val_log)\n",
        "dfull  = xgb.DMatrix(X_full_mat, label=y_full_log)\n",
        "dtest  = xgb.DMatrix(X_test_mat)\n",
        "\n",
        "xgb_params = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"eval_metric\": \"rmse\",\n",
        "    \"tree_method\": XGB_TREE_METHOD,\n",
        "    \"seed\": SEED,\n",
        "    **XGB_PARAMS\n",
        "}\n",
        "\n",
        "print(\"\\nTraining Economy -> XGBoost (early stopping) ...\")\n",
        "booster = xgb.train(\n",
        "    params=xgb_params,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=XGB_NUM_BOOST_ROUND,\n",
        "    evals=[(dval, \"val\")],\n",
        "    early_stopping_rounds=XGB_EARLY_STOPPING_ROUNDS,\n",
        "    verbose_eval=200\n",
        ")\n",
        "\n",
        "best_it = int(booster.best_iteration)\n",
        "\n",
        "train_pred_log = booster.predict(dfull, iteration_range=(0, best_it + 1))\n",
        "test_pred_log  = booster.predict(dtest, iteration_range=(0, best_it + 1))\n",
        "\n",
        "pred_train = np.clip(np.expm1(train_pred_log), 0, None)\n",
        "pred_test  = np.clip(np.expm1(test_pred_log),  0, None)\n",
        "\n",
        "mt = compute_metrics(y_full, pred_train)\n",
        "ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "hybrid_models[\"Economy\"] = {\n",
        "    \"type\": \"xgb\",\n",
        "    \"pre\": pre,\n",
        "    \"booster\": booster,\n",
        "    \"best_iter\": best_it,\n",
        "    \"features\": X_full.columns.tolist()\n",
        "}\n",
        "\n",
        "results.append({\n",
        "    \"class\": \"Economy\",\n",
        "    \"model\": \"XGBoost(native)\",\n",
        "    \"best_iteration\": best_it,\n",
        "    \"train_RMSE\": mt[\"RMSE\"], \"train_MAE\": mt[\"MAE\"], \"train_R2\": mt[\"R2\"],\n",
        "    \"test_RMSE\":  ms[\"RMSE\"], \"test_MAE\":  ms[\"MAE\"], \"test_R2\":  ms[\"R2\"],\n",
        "})\n",
        "\n",
        "# ============================================================\n",
        "# 4) Print summary\n",
        "# ============================================================\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n===== HYBRID MODEL RESULTS =====\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5) Predict helper for NEW DATA\n",
        "#    new_df must include the same raw columns (except price)\n",
        "# ============================================================\n",
        "def predict_price_hybrid(new_df: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Predict price for new rows using the correct model based on new_df['class'].\n",
        "    Returns numpy array of predicted price on original scale.\n",
        "    \"\"\"\n",
        "    if \"class\" not in new_df.columns:\n",
        "        raise ValueError(\"new_df must contain a 'class' column (Business/Economy).\")\n",
        "\n",
        "    preds = np.empty(len(new_df), dtype=float)\n",
        "    preds[:] = np.nan\n",
        "\n",
        "    for cls in [\"Business\", \"Economy\"]:\n",
        "        mask = (new_df[\"class\"] == cls).values\n",
        "        if not mask.any():\n",
        "            continue\n",
        "\n",
        "        sub = new_df.loc[mask].copy()\n",
        "\n",
        "        # drop flight if exists\n",
        "        if \"flight\" in sub.columns:\n",
        "            sub = sub.drop(columns=[\"flight\"])\n",
        "\n",
        "        # drop price if accidentally present\n",
        "        if \"price\" in sub.columns:\n",
        "            sub = sub.drop(columns=[\"price\"])\n",
        "\n",
        "        # add engineered features\n",
        "        sub = add_interactions(sub)\n",
        "\n",
        "        if cls == \"Business\":\n",
        "            cols = hybrid_models[\"Business\"][\"features\"]\n",
        "            sub = sub.reindex(columns=cols)\n",
        "            log_pred = hybrid_models[\"Business\"][\"model\"].predict(sub)\n",
        "            preds[mask] = np.clip(np.expm1(log_pred), 0, None)\n",
        "\n",
        "        else:\n",
        "            cols = hybrid_models[\"Economy\"][\"features\"]\n",
        "            sub = sub.reindex(columns=cols)\n",
        "\n",
        "            pre = hybrid_models[\"Economy\"][\"pre\"]\n",
        "            booster = hybrid_models[\"Economy\"][\"booster\"]\n",
        "            best_it = hybrid_models[\"Economy\"][\"best_iter\"]\n",
        "\n",
        "            X = to_csr(pre.transform(sub))\n",
        "            d = xgb.DMatrix(X)\n",
        "            log_pred = booster.predict(d, iteration_range=(0, best_it + 1))\n",
        "            preds[mask] = np.clip(np.expm1(log_pred), 0, None)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# Example usage:\n",
        "# new_prices = predict_price_hybrid(new_data_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "ff4443e9ec2d42bb8f7eaeebacf9bdc0",
            "3549b2d956fb418c9e182810c68978cf",
            "aade1f0307614af8ab826a91c71c1b81",
            "cad660217ab14b8099856369014fb1fe",
            "648bd30cefb84286a424f80bdbb03aee",
            "dc5006a96514428888ddfa418130135e",
            "8590a8c23da545499fb3cf6b25579aed",
            "b9a8831974244db388571d1c5146a0e0",
            "2dad27c5056b43a995ed946410b3b817",
            "e8dcce2766174f919067b3346e26e048",
            "d76a3ec87f7643988e2e62927c979b77"
          ]
        },
        "id": "D2aB6OYVXtZo",
        "outputId": "6e01ada1-85a1-424c-dc8f-2ad71b65c4fb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff4443e9ec2d42bb8f7eaeebacf9bdc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Classes:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fitting AdaBoost for class=Business ...\n",
            "\n",
            "Fitting AdaBoost for class=Economy ...\n",
            "\n",
            "===== ADABOOST RESULTS (separate model per class | same split) =====\n",
            "   class  n_train  n_test  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2\n",
            "Business    24917    6229 8286.901657 6472.925513  0.590003 8279.299060 6512.461385 0.573996\n",
            " Economy    55083   13771 2121.977002 1445.053433  0.677196 2144.967393 1448.526161 0.668285\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# ADABOOST REGRESSOR (separate model per class) + progress bar\n",
        "# - SAME train/test split via train_idx/test_idx (fair comparison)\n",
        "# - Drop flight\n",
        "# - log1p target, expm1 back to price for metrics\n",
        "# - Adds route + route_airline features\n",
        "# Assumes df is already loaded\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "# AdaBoost settings (start reasonable; you can tune later)\n",
        "N_ESTIMATORS = 400\n",
        "LEARNING_RATE = 0.05\n",
        "BASE_MAX_DEPTH = 6\n",
        "BASE_MIN_SAMPLES_LEAF = 10\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def add_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    X = add_interactions(X)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------\n",
        "# SAME split as your other models (fallback creates if missing)\n",
        "# -------------------------------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Train AdaBoost separately per class\n",
        "# -------------------------------------------------\n",
        "rows = []\n",
        "models = {}\n",
        "\n",
        "for cls in tqdm(sorted(df[\"class\"].dropna().unique()), desc=\"Classes\"):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_train, y_train, y_train_log = prepare_X_y(train_sub)\n",
        "    X_test,  y_test,  _           = prepare_X_y(test_sub)\n",
        "\n",
        "    # Align columns\n",
        "    X_test = X_test.reindex(columns=X_train.columns)\n",
        "\n",
        "    pre = build_preprocessor(X_train)\n",
        "\n",
        "    base_tree = DecisionTreeRegressor(\n",
        "        max_depth=BASE_MAX_DEPTH,\n",
        "        min_samples_leaf=BASE_MIN_SAMPLES_LEAF,\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    ada = AdaBoostRegressor(\n",
        "        estimator=base_tree,         # use 'base_estimator' if your sklearn is older\n",
        "        n_estimators=N_ESTIMATORS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        loss=\"linear\",\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"pre\", pre),\n",
        "        (\"model\", ada)\n",
        "    ])\n",
        "\n",
        "    print(f\"\\nFitting AdaBoost for class={cls} ...\")\n",
        "    pipe.fit(X_train, y_train_log)\n",
        "\n",
        "    pred_train = np.clip(np.expm1(pipe.predict(X_train)), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(pipe.predict(X_test)),  0, None)\n",
        "\n",
        "    mt = compute_metrics(y_train, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    models[cls] = pipe\n",
        "    rows.append({\n",
        "        \"class\": cls,\n",
        "        \"n_train\": len(X_train),\n",
        "        \"n_test\": len(X_test),\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"train_MAE\":  mt[\"MAE\"],\n",
        "        \"train_R2\":   mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"],\n",
        "        \"test_MAE\":   ms[\"MAE\"],\n",
        "        \"test_R2\":    ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows).sort_values(\"class\")\n",
        "print(\"\\n===== ADABOOST RESULTS (separate model per class | same split) =====\")\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GyGkzJcKi1g",
        "outputId": "99d359c7-329d-4d23-a9a8-bb55350ee67f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Business -> RandomForest (regularized to reduce gap) ...\n",
            "\n",
            "Training Economy -> XGBoost(native) with early stopping ...\n",
            "[0]\tval-rmse:0.50349\n",
            "[200]\tval-rmse:0.19345\n",
            "[400]\tval-rmse:0.18339\n",
            "[600]\tval-rmse:0.17689\n",
            "[800]\tval-rmse:0.17348\n",
            "[1000]\tval-rmse:0.17113\n",
            "[1200]\tval-rmse:0.16962\n",
            "[1400]\tval-rmse:0.16868\n",
            "[1600]\tval-rmse:0.16774\n",
            "[1800]\tval-rmse:0.16732\n",
            "[2000]\tval-rmse:0.16685\n",
            "[2200]\tval-rmse:0.16661\n",
            "[2400]\tval-rmse:0.16631\n",
            "[2600]\tval-rmse:0.16618\n",
            "[2800]\tval-rmse:0.16606\n",
            "[2994]\tval-rmse:0.16614\n",
            "\n",
            "===== HYBRID RESULTS (ONLY Business RF modified) =====\n",
            "   class                  model  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2    gap_MAE   gap_RMSE  best_iteration\n",
            "Business RandomForest(modified) 5026.291157 3197.807039  0.849169 5326.558856 3440.591157 0.823673 242.784118 300.267698             NaN\n",
            " Economy        XGBoost(native)  895.926243  444.355826  0.942456 1504.131592  748.190187 0.836884 303.834361 608.205349          2794.0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# HYBRID (keep Economy as-is) + ONLY change Business RandomForest to reduce gap\n",
        "#   Business -> RandomForest (regularized to reduce overfit gap)\n",
        "#   Economy  -> XGBoost(native) (same approach as before: early stopping)\n",
        "#\n",
        "# - SAME train/test split via train_idx/test_idx\n",
        "# - Drop flight\n",
        "# - log1p target transform, expm1 back to price for metrics\n",
        "# - Adds route + route_airline features\n",
        "#\n",
        "# Assumes df is already loaded\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "import xgboost as xgb\n",
        "from scipy import sparse\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "# -----------------------------\n",
        "# ONLY CHANGE: Business RF regularization (gap reduction)\n",
        "# -----------------------------\n",
        "BUSINESS_RF_PARAMS = dict(\n",
        "    n_estimators=600,        # 500-800 is fine\n",
        "    max_depth=25,            # cap depth (reduces overfit)\n",
        "    min_samples_leaf=10,     # bigger leaves (reduces overfit)\n",
        "    min_samples_split=20,    # less aggressive splits\n",
        "    max_features=0.8,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Economy XGBoost (keep as-is)\n",
        "# (same style you used: native train + early stopping)\n",
        "# -----------------------------\n",
        "XGB_PARAMS = {\n",
        "    \"eta\": 0.05,\n",
        "    \"max_depth\": 10,\n",
        "    \"min_child_weight\": 5,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"lambda\": 2.0,\n",
        "    \"alpha\": 0.0,\n",
        "    \"gamma\": 0.0,\n",
        "}\n",
        "\n",
        "XGB_NUM_BOOST_ROUND = 20000\n",
        "XGB_EARLY_STOPPING_ROUNDS = 200\n",
        "XGB_TREE_METHOD = \"hist\"\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def add_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    X = add_interactions(X)\n",
        "    return X, y, y_log\n",
        "\n",
        "def get_feature_lists(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "    return num_cols, cat_cols\n",
        "\n",
        "def make_onehot_preprocessor(num_cols, cat_cols):\n",
        "    num_pipe = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "\n",
        "def to_csr(X):\n",
        "    return X.tocsr() if sparse.issparse(X) else X\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) Same split\n",
        "# ============================================================\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) BUSINESS -> RandomForest (modified to reduce gap)\n",
        "# ============================================================\n",
        "cls = \"Business\"\n",
        "train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "X_train, y_train, y_train_log = prepare_X_y(train_sub)\n",
        "X_test,  y_test,  _           = prepare_X_y(test_sub)\n",
        "X_test = X_test.reindex(columns=X_train.columns)\n",
        "\n",
        "num_cols, cat_cols = get_feature_lists(X_train)\n",
        "pre_b = make_onehot_preprocessor(num_cols, cat_cols)\n",
        "\n",
        "rf = RandomForestRegressor(**BUSINESS_RF_PARAMS)\n",
        "rf_pipe = Pipeline(steps=[(\"pre\", pre_b), (\"model\", rf)])\n",
        "\n",
        "print(\"\\nTraining Business -> RandomForest (regularized to reduce gap) ...\")\n",
        "rf_pipe.fit(X_train, y_train_log)\n",
        "\n",
        "pred_train_b = np.clip(np.expm1(rf_pipe.predict(X_train)), 0, None)\n",
        "pred_test_b  = np.clip(np.expm1(rf_pipe.predict(X_test)),  0, None)\n",
        "\n",
        "mt_b = compute_metrics(y_train, pred_train_b)\n",
        "ms_b = compute_metrics(y_test, pred_test_b)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) ECONOMY -> XGBoost(native) (keep as-is)\n",
        "# ============================================================\n",
        "cls = \"Economy\"\n",
        "train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "X_full, y_full, y_full_log = prepare_X_y(train_sub)\n",
        "X_test_df, y_test_e, _     = prepare_X_y(test_sub)\n",
        "X_test_df = X_test_df.reindex(columns=X_full.columns)\n",
        "\n",
        "# internal train/val split for early stopping\n",
        "X_tr_df, X_val_df, y_tr, y_val, y_tr_log, y_val_log = train_test_split(\n",
        "    X_full, y_full, y_full_log,\n",
        "    test_size=VAL_SIZE,\n",
        "    random_state=SEED,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "num_cols, cat_cols = get_feature_lists(X_tr_df)\n",
        "pre_e = make_onehot_preprocessor(num_cols, cat_cols)\n",
        "\n",
        "X_tr  = to_csr(pre_e.fit_transform(X_tr_df))\n",
        "X_val = to_csr(pre_e.transform(X_val_df))\n",
        "X_full_mat = to_csr(pre_e.transform(X_full))\n",
        "X_test_mat = to_csr(pre_e.transform(X_test_df))\n",
        "\n",
        "dtrain = xgb.DMatrix(X_tr, label=y_tr_log)\n",
        "dval   = xgb.DMatrix(X_val, label=y_val_log)\n",
        "dfull  = xgb.DMatrix(X_full_mat, label=y_full_log)\n",
        "dtest  = xgb.DMatrix(X_test_mat)\n",
        "\n",
        "xgb_params = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"eval_metric\": \"rmse\",\n",
        "    \"tree_method\": XGB_TREE_METHOD,\n",
        "    \"seed\": SEED,\n",
        "    **XGB_PARAMS\n",
        "}\n",
        "\n",
        "print(\"\\nTraining Economy -> XGBoost(native) with early stopping ...\")\n",
        "booster = xgb.train(\n",
        "    params=xgb_params,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=XGB_NUM_BOOST_ROUND,\n",
        "    evals=[(dval, \"val\")],\n",
        "    early_stopping_rounds=XGB_EARLY_STOPPING_ROUNDS,\n",
        "    verbose_eval=200\n",
        ")\n",
        "\n",
        "best_it = int(booster.best_iteration)\n",
        "\n",
        "train_pred_log_e = booster.predict(dfull, iteration_range=(0, best_it + 1))\n",
        "test_pred_log_e  = booster.predict(dtest, iteration_range=(0, best_it + 1))\n",
        "\n",
        "pred_train_e = np.clip(np.expm1(train_pred_log_e), 0, None)\n",
        "pred_test_e  = np.clip(np.expm1(test_pred_log_e),  0, None)\n",
        "\n",
        "mt_e = compute_metrics(y_full, pred_train_e)\n",
        "ms_e = compute_metrics(y_test_e, pred_test_e)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) Summary table + gap\n",
        "# ============================================================\n",
        "summary = pd.DataFrame([\n",
        "    {\n",
        "        \"class\": \"Business\",\n",
        "        \"model\": \"RandomForest(modified)\",\n",
        "        \"train_RMSE\": mt_b[\"RMSE\"], \"train_MAE\": mt_b[\"MAE\"], \"train_R2\": mt_b[\"R2\"],\n",
        "        \"test_RMSE\":  ms_b[\"RMSE\"], \"test_MAE\":  ms_b[\"MAE\"], \"test_R2\":  ms_b[\"R2\"],\n",
        "        \"gap_MAE\": ms_b[\"MAE\"] - mt_b[\"MAE\"],\n",
        "        \"gap_RMSE\": ms_b[\"RMSE\"] - mt_b[\"RMSE\"],\n",
        "        \"best_iteration\": np.nan,\n",
        "    },\n",
        "    {\n",
        "        \"class\": \"Economy\",\n",
        "        \"model\": \"XGBoost(native)\",\n",
        "        \"train_RMSE\": mt_e[\"RMSE\"], \"train_MAE\": mt_e[\"MAE\"], \"train_R2\": mt_e[\"R2\"],\n",
        "        \"test_RMSE\":  ms_e[\"RMSE\"], \"test_MAE\":  ms_e[\"MAE\"], \"test_R2\":  ms_e[\"R2\"],\n",
        "        \"gap_MAE\": ms_e[\"MAE\"] - mt_e[\"MAE\"],\n",
        "        \"gap_RMSE\": ms_e[\"RMSE\"] - mt_e[\"RMSE\"],\n",
        "        \"best_iteration\": best_it,\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"\\n===== HYBRID RESULTS (ONLY Business RF modified) =====\")\n",
        "print(summary.to_string(index=False))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5) Predict helper for NEW DATA\n",
        "# ============================================================\n",
        "def predict_price_hybrid(new_df: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Predict price for new rows using:\n",
        "      - Business -> modified RF\n",
        "      - Economy  -> XGBoost(native)\n",
        "    Returns predictions on ORIGINAL price scale.\n",
        "    \"\"\"\n",
        "    if \"class\" not in new_df.columns:\n",
        "        raise ValueError(\"new_df must contain a 'class' column (Business/Economy).\")\n",
        "\n",
        "    preds = np.empty(len(new_df), dtype=float)\n",
        "    preds[:] = np.nan\n",
        "\n",
        "    # Common cleanup + feature add\n",
        "    def prep_new(sub):\n",
        "        sub = sub.copy()\n",
        "        if \"flight\" in sub.columns:\n",
        "            sub = sub.drop(columns=[\"flight\"])\n",
        "        if \"price\" in sub.columns:\n",
        "            sub = sub.drop(columns=[\"price\"])\n",
        "        sub = add_interactions(sub)\n",
        "        return sub\n",
        "\n",
        "    # Business\n",
        "    mask_b = (new_df[\"class\"] == \"Business\").values\n",
        "    if mask_b.any():\n",
        "        sub = prep_new(new_df.loc[mask_b])\n",
        "        # align columns to training\n",
        "        sub = sub.reindex(columns=X_train.columns)\n",
        "        log_pred = rf_pipe.predict(sub)\n",
        "        preds[mask_b] = np.clip(np.expm1(log_pred), 0, None)\n",
        "\n",
        "    # Economy\n",
        "    mask_e = (new_df[\"class\"] == \"Economy\").values\n",
        "    if mask_e.any():\n",
        "        sub = prep_new(new_df.loc[mask_e])\n",
        "        sub = sub.reindex(columns=X_full.columns)\n",
        "        Xn = to_csr(pre_e.transform(sub))\n",
        "        dn = xgb.DMatrix(Xn)\n",
        "        log_pred = booster.predict(dn, iteration_range=(0, best_it + 1))\n",
        "        preds[mask_e] = np.clip(np.expm1(log_pred), 0, None)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# Example:\n",
        "# preds = predict_price_hybrid(new_data_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "92df9f15ca1b4c0f8ea7f7efb34f0951",
            "136bb11afd9740a2bbf084e3281a0b71",
            "de9a9e0cfa354a78950c6cfda164c7e4",
            "18dde0a0796c4a63b5f4bf8f4810aea9",
            "d4656de4623d47acb3a229679a7cf792",
            "13dd6996ca9f461bbdd62204400c3d61",
            "ef3df5ccbd6946aeb8ef536e7bbb6aff",
            "50aac5fcc3794491b4309eee1e32c0fe",
            "aa536d160cfb4c9e884a3cb6ea5ed777",
            "f28dd31a582140889e80d668cd56c955",
            "a32ab4a726f34811bea3f2b673a489b3"
          ]
        },
        "id": "AbUup7MTZiy8",
        "outputId": "e2b22b94-3aa9-45ba-b61d-91e787e2f089"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92df9f15ca1b4c0f8ea7f7efb34f0951",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Business RF candidates:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== BUSINESS RF SWEEP (sorted by test_MAE then gap_MAE) =====\n",
            " max_depth  min_leaf  min_split  max_samples   train_MAE    test_MAE    gap_MAE  train_RMSE   test_RMSE    gap_RMSE  test_R2\n",
            "       NaN         1          5          1.0 1361.949937 2360.644196 998.694259 2675.019927 4495.956847 1820.936920 0.874377\n",
            "      40.0         2          8          0.9 1839.605554 2476.002046 636.396493 3437.893433 4501.634511 1063.741078 0.874059\n",
            "      35.0         3         10          0.8 2157.299400 2644.551019 487.251619 3849.642348 4604.470693  754.828345 0.868239\n",
            "      30.0         5         10          0.8 2469.499866 2883.256823 413.756957 4202.335323 4800.137283  597.801960 0.856803\n",
            "      30.0         5         20          0.7 2930.353840 3191.517801 261.163961 4723.617690 5061.095339  337.477649 0.840810\n",
            "      25.0         8         20          0.8 3172.707254 3417.040331 244.333077 5000.584588 5297.609492  297.024903 0.825584\n",
            "\n",
            "Best candidate (Business):\n",
            "max_depth              NaN\n",
            "min_leaf          1.000000\n",
            "min_split         5.000000\n",
            "max_samples       1.000000\n",
            "train_MAE      1361.949937\n",
            "test_MAE       2360.644196\n",
            "gap_MAE         998.694259\n",
            "train_RMSE     2675.019927\n",
            "test_RMSE      4495.956847\n",
            "gap_RMSE       1820.936920\n",
            "test_R2           0.874377\n",
            "Name: 0, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    return train_test_split(\n",
        "        idx, test_size=TEST_SIZE, random_state=SEED, shuffle=True, stratify=df[\"class\"]\n",
        "    )\n",
        "\n",
        "def add_interactions(X):\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "    X = add_interactions(X)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
        "    cat_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        [(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "\n",
        "# --- reuse SAME split indices ---\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# ============================================================\n",
        "# BUSINESS ONLY: fast RF sweep around the “sweet spot”\n",
        "# ============================================================\n",
        "train_sub = df_train[df_train[\"class\"] == \"Business\"].copy()\n",
        "test_sub  = df_test[df_test[\"class\"] == \"Business\"].copy()\n",
        "\n",
        "X_train, y_train, y_train_log = prepare_X_y(train_sub)\n",
        "X_test,  y_test,  _           = prepare_X_y(test_sub)\n",
        "X_test = X_test.reindex(columns=X_train.columns)\n",
        "\n",
        "pre = build_preprocessor(X_train)\n",
        "\n",
        "# Fast candidate configs (middle ground between your old and new)\n",
        "candidates = [\n",
        "    # (depth, leaf, split, max_samples)\n",
        "    (None, 1, 5, 1.0),        # old-like (baseline)\n",
        "    (40,   2, 8, 0.9),\n",
        "    (35,   3, 10, 0.8),       # recommended balance\n",
        "    (30,   5, 10, 0.8),\n",
        "    (30,   5, 20, 0.7),\n",
        "    (25,   8, 20, 0.8),       # closer to your over-regularized version but less extreme\n",
        "]\n",
        "\n",
        "N_ESTIMATORS = 600  # fast; raise to 800-1200 after picking config\n",
        "\n",
        "rows = []\n",
        "for max_depth, min_leaf, min_split, max_samples in tqdm(candidates, desc=\"Business RF candidates\"):\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=N_ESTIMATORS,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1,\n",
        "        bootstrap=True,\n",
        "        max_features=0.8,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_leaf=min_leaf,\n",
        "        min_samples_split=min_split,\n",
        "        max_samples=max_samples\n",
        "    )\n",
        "    pipe = Pipeline([(\"pre\", pre), (\"model\", rf)])\n",
        "    pipe.fit(X_train, y_train_log)\n",
        "\n",
        "    pred_tr = np.clip(np.expm1(pipe.predict(X_train)), 0, None)\n",
        "    pred_te = np.clip(np.expm1(pipe.predict(X_test)),  0, None)\n",
        "\n",
        "    mt = metrics(y_train, pred_tr)\n",
        "    ms = metrics(y_test, pred_te)\n",
        "\n",
        "    rows.append({\n",
        "        \"max_depth\": max_depth,\n",
        "        \"min_leaf\": min_leaf,\n",
        "        \"min_split\": min_split,\n",
        "        \"max_samples\": max_samples,\n",
        "        \"train_MAE\": mt[\"MAE\"],\n",
        "        \"test_MAE\": ms[\"MAE\"],\n",
        "        \"gap_MAE\": ms[\"MAE\"] - mt[\"MAE\"],\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"test_RMSE\": ms[\"RMSE\"],\n",
        "        \"gap_RMSE\": ms[\"RMSE\"] - mt[\"RMSE\"],\n",
        "        \"test_R2\": ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "res = pd.DataFrame(rows).sort_values([\"test_MAE\", \"gap_MAE\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n===== BUSINESS RF SWEEP (sorted by test_MAE then gap_MAE) =====\")\n",
        "print(res.to_string(index=False))\n",
        "\n",
        "best = res.iloc[0]\n",
        "print(\"\\nBest candidate (Business):\")\n",
        "print(best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmMn-pvecg6V",
        "outputId": "b2f38158-632c-4893-c6fa-78376a78a09a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Business -> RandomForest (compromise config) ...\n",
            "\n",
            "Training Economy -> XGBoost(native) with early stopping (as-is) ...\n",
            "[0]\tval-rmse:0.50349\n",
            "[200]\tval-rmse:0.19345\n",
            "[400]\tval-rmse:0.18339\n",
            "[600]\tval-rmse:0.17689\n",
            "[800]\tval-rmse:0.17348\n",
            "[1000]\tval-rmse:0.17113\n",
            "[1200]\tval-rmse:0.16962\n",
            "[1400]\tval-rmse:0.16868\n",
            "[1600]\tval-rmse:0.16774\n",
            "[1800]\tval-rmse:0.16732\n",
            "[2000]\tval-rmse:0.16685\n",
            "[2200]\tval-rmse:0.16661\n",
            "[2400]\tval-rmse:0.16631\n",
            "[2600]\tval-rmse:0.16618\n",
            "[2800]\tval-rmse:0.16606\n",
            "[2994]\tval-rmse:0.16614\n",
            "\n",
            "===== HYBRID RESULTS (Business RF compromise + Economy XGB as-is) =====\n",
            "   class                    model  n_train  n_test  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2    gap_MAE    gap_RMSE  best_iteration\n",
            "Business RandomForest(compromise)    24917    6229 3437.301737 1839.468156  0.929461 4501.751501 2476.041322 0.874053 636.573165 1064.449764             NaN\n",
            " Economy          XGBoost(native)    55083   13771  895.926243  444.355826  0.942456 1504.131592  748.190187 0.836884 303.834361  608.205349          2794.0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# HYBRID MODEL (as requested)\n",
        "#   Business -> RandomForest with your best compromise params:\n",
        "#              max_depth=40, min_leaf=2, min_split=8, max_samples=0.9\n",
        "#   Economy  -> XGBoost(native) KEEP AS-IS (same params + early stopping)\n",
        "#\n",
        "# - SAME train/test split via train_idx/test_idx\n",
        "# - Drop flight\n",
        "# - log1p target transform, expm1 back to price for metrics\n",
        "# - Adds route + route_airline features\n",
        "#\n",
        "# Assumes df is already loaded\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "import xgboost as xgb\n",
        "from scipy import sparse\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "# -----------------------------\n",
        "# Business RF (compromise config you selected)\n",
        "# -----------------------------\n",
        "BUSINESS_RF = dict(\n",
        "    n_estimators=800,     # raise/lower for speed\n",
        "    max_depth=40,\n",
        "    min_samples_leaf=2,\n",
        "    min_samples_split=8,\n",
        "    max_features=0.8,\n",
        "    bootstrap=True,\n",
        "    max_samples=0.9,      # key for variance reduction\n",
        "    n_jobs=-1,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Economy XGBoost (KEEP AS-IS)\n",
        "# -----------------------------\n",
        "XGB_PARAMS = {\n",
        "    \"eta\": 0.05,\n",
        "    \"max_depth\": 10,\n",
        "    \"min_child_weight\": 5,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"lambda\": 2.0,\n",
        "    \"alpha\": 0.0,\n",
        "    \"gamma\": 0.0,\n",
        "}\n",
        "XGB_NUM_BOOST_ROUND = 20000\n",
        "XGB_EARLY_STOPPING_ROUNDS = 200\n",
        "XGB_TREE_METHOD = \"hist\"\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def add_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def prepare_X_y(df_sub: pd.DataFrame):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    X = add_interactions(X)\n",
        "    return X, y, y_log\n",
        "\n",
        "def get_feature_lists(X: pd.DataFrame):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "    return num_cols, cat_cols\n",
        "\n",
        "def make_onehot_preprocessor(num_cols, cat_cols):\n",
        "    num_pipe = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "\n",
        "def to_csr(X):\n",
        "    return X.tocsr() if sparse.issparse(X) else X\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) Same split indices (reuse if already created)\n",
        "# ============================================================\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) BUSINESS -> RandomForest (compromise params)\n",
        "# ============================================================\n",
        "train_b = df_train[df_train[\"class\"] == \"Business\"].copy()\n",
        "test_b  = df_test[df_test[\"class\"] == \"Business\"].copy()\n",
        "\n",
        "Xb_train, yb_train, yb_train_log = prepare_X_y(train_b)\n",
        "Xb_test,  yb_test,  _            = prepare_X_y(test_b)\n",
        "Xb_test = Xb_test.reindex(columns=Xb_train.columns)\n",
        "\n",
        "num_cols_b, cat_cols_b = get_feature_lists(Xb_train)\n",
        "pre_b = make_onehot_preprocessor(num_cols_b, cat_cols_b)\n",
        "\n",
        "rf = RandomForestRegressor(**BUSINESS_RF)\n",
        "rf_pipe = Pipeline(steps=[(\"pre\", pre_b), (\"model\", rf)])\n",
        "\n",
        "print(\"\\nTraining Business -> RandomForest (compromise config) ...\")\n",
        "rf_pipe.fit(Xb_train, yb_train_log)\n",
        "\n",
        "pred_train_b = np.clip(np.expm1(rf_pipe.predict(Xb_train)), 0, None)\n",
        "pred_test_b  = np.clip(np.expm1(rf_pipe.predict(Xb_test)),  0, None)\n",
        "\n",
        "mt_b = compute_metrics(yb_train, pred_train_b)\n",
        "ms_b = compute_metrics(yb_test, pred_test_b)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) ECONOMY -> XGBoost(native) (keep as-is)\n",
        "# ============================================================\n",
        "train_e = df_train[df_train[\"class\"] == \"Economy\"].copy()\n",
        "test_e  = df_test[df_test[\"class\"] == \"Economy\"].copy()\n",
        "\n",
        "Xe_full, ye_full, ye_full_log = prepare_X_y(train_e)\n",
        "Xe_test_df, ye_test, _        = prepare_X_y(test_e)\n",
        "Xe_test_df = Xe_test_df.reindex(columns=Xe_full.columns)\n",
        "\n",
        "# internal train/val split for early stopping\n",
        "Xe_tr_df, Xe_val_df, ye_tr, ye_val, ye_tr_log, ye_val_log = train_test_split(\n",
        "    Xe_full, ye_full, ye_full_log,\n",
        "    test_size=VAL_SIZE,\n",
        "    random_state=SEED,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "num_cols_e, cat_cols_e = get_feature_lists(Xe_tr_df)\n",
        "pre_e = make_onehot_preprocessor(num_cols_e, cat_cols_e)\n",
        "\n",
        "Xe_tr   = to_csr(pre_e.fit_transform(Xe_tr_df))\n",
        "Xe_val  = to_csr(pre_e.transform(Xe_val_df))\n",
        "Xe_full_mat = to_csr(pre_e.transform(Xe_full))\n",
        "Xe_test_mat = to_csr(pre_e.transform(Xe_test_df))\n",
        "\n",
        "dtrain = xgb.DMatrix(Xe_tr, label=ye_tr_log)\n",
        "dval   = xgb.DMatrix(Xe_val, label=ye_val_log)\n",
        "dfull  = xgb.DMatrix(Xe_full_mat, label=ye_full_log)\n",
        "dtest  = xgb.DMatrix(Xe_test_mat)\n",
        "\n",
        "xgb_params = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"eval_metric\": \"rmse\",\n",
        "    \"tree_method\": XGB_TREE_METHOD,\n",
        "    \"seed\": SEED,\n",
        "    **XGB_PARAMS\n",
        "}\n",
        "\n",
        "print(\"\\nTraining Economy -> XGBoost(native) with early stopping (as-is) ...\")\n",
        "booster = xgb.train(\n",
        "    params=xgb_params,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=XGB_NUM_BOOST_ROUND,\n",
        "    evals=[(dval, \"val\")],\n",
        "    early_stopping_rounds=XGB_EARLY_STOPPING_ROUNDS,\n",
        "    verbose_eval=200\n",
        ")\n",
        "\n",
        "best_it = int(booster.best_iteration)\n",
        "\n",
        "pred_train_e = np.clip(np.expm1(booster.predict(dfull, iteration_range=(0, best_it + 1))), 0, None)\n",
        "pred_test_e  = np.clip(np.expm1(booster.predict(dtest, iteration_range=(0, best_it + 1))), 0, None)\n",
        "\n",
        "mt_e = compute_metrics(ye_full, pred_train_e)\n",
        "ms_e = compute_metrics(ye_test, pred_test_e)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) Summary\n",
        "# ============================================================\n",
        "summary = pd.DataFrame([\n",
        "    {\n",
        "        \"class\": \"Business\",\n",
        "        \"model\": \"RandomForest(compromise)\",\n",
        "        \"n_train\": len(Xb_train),\n",
        "        \"n_test\": len(Xb_test),\n",
        "        \"train_RMSE\": mt_b[\"RMSE\"], \"train_MAE\": mt_b[\"MAE\"], \"train_R2\": mt_b[\"R2\"],\n",
        "        \"test_RMSE\":  ms_b[\"RMSE\"], \"test_MAE\":  ms_b[\"MAE\"], \"test_R2\":  ms_b[\"R2\"],\n",
        "        \"gap_MAE\": ms_b[\"MAE\"] - mt_b[\"MAE\"],\n",
        "        \"gap_RMSE\": ms_b[\"RMSE\"] - mt_b[\"RMSE\"],\n",
        "        \"best_iteration\": np.nan\n",
        "    },\n",
        "    {\n",
        "        \"class\": \"Economy\",\n",
        "        \"model\": \"XGBoost(native)\",\n",
        "        \"n_train\": len(Xe_full),\n",
        "        \"n_test\": len(Xe_test_df),\n",
        "        \"train_RMSE\": mt_e[\"RMSE\"], \"train_MAE\": mt_e[\"MAE\"], \"train_R2\": mt_e[\"R2\"],\n",
        "        \"test_RMSE\":  ms_e[\"RMSE\"], \"test_MAE\":  ms_e[\"MAE\"], \"test_R2\":  ms_e[\"R2\"],\n",
        "        \"gap_MAE\": ms_e[\"MAE\"] - mt_e[\"MAE\"],\n",
        "        \"gap_RMSE\": ms_e[\"RMSE\"] - mt_e[\"RMSE\"],\n",
        "        \"best_iteration\": best_it\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"\\n===== HYBRID RESULTS (Business RF compromise + Economy XGB as-is) =====\")\n",
        "print(summary.to_string(index=False))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5) Predict helper for NEW DATA\n",
        "# ============================================================\n",
        "def predict_price_hybrid(new_df: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Predict price for new rows using:\n",
        "      - Business -> RF compromise model\n",
        "      - Economy  -> XGBoost(native) as-is (best_it)\n",
        "    Returns predictions on ORIGINAL price scale.\n",
        "    \"\"\"\n",
        "    if \"class\" not in new_df.columns:\n",
        "        raise ValueError(\"new_df must contain a 'class' column (Business/Economy).\")\n",
        "\n",
        "    preds = np.full(len(new_df), np.nan, dtype=float)\n",
        "\n",
        "    def prep_new(sub: pd.DataFrame) -> pd.DataFrame:\n",
        "        sub = sub.copy()\n",
        "        if \"flight\" in sub.columns:\n",
        "            sub = sub.drop(columns=[\"flight\"])\n",
        "        if \"price\" in sub.columns:\n",
        "            sub = sub.drop(columns=[\"price\"])\n",
        "        sub = add_interactions(sub)\n",
        "        return sub\n",
        "\n",
        "    # Business\n",
        "    mask_b = (new_df[\"class\"] == \"Business\").values\n",
        "    if mask_b.any():\n",
        "        sub_b = prep_new(new_df.loc[mask_b])\n",
        "        sub_b = sub_b.reindex(columns=Xb_train.columns)\n",
        "        log_pred_b = rf_pipe.predict(sub_b)\n",
        "        preds[mask_b] = np.clip(np.expm1(log_pred_b), 0, None)\n",
        "\n",
        "    # Economy\n",
        "    mask_e = (new_df[\"class\"] == \"Economy\").values\n",
        "    if mask_e.any():\n",
        "        sub_e = prep_new(new_df.loc[mask_e])\n",
        "        sub_e = sub_e.reindex(columns=Xe_full.columns)\n",
        "        Xe_new = to_csr(pre_e.transform(sub_e))\n",
        "        dnew = xgb.DMatrix(Xe_new)\n",
        "        log_pred_e = booster.predict(dnew, iteration_range=(0, best_it + 1))\n",
        "        preds[mask_e] = np.clip(np.expm1(log_pred_e), 0, None)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# Example:\n",
        "# preds = predict_price_hybrid(new_data_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ae03a4c",
        "outputId": "bfc6ce6c-31e3-48c9-e847-30151483303e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest (Business Class) Train MAPE: 3.29%\n",
            "Random Forest (Business Class) Test MAPE: 4.53%\n"
          ]
        }
      ],
      "source": [
        "def mape(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Avoid division by zero by replacing 0 with a small epsilon\n",
        "    epsilon = np.finfo(np.float64).eps\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, epsilon))) * 100\n",
        "\n",
        "train_mape_rf_b = mape(yb_train, pred_train_b)\n",
        "test_mape_rf_b = mape(yb_test, pred_test_b)\n",
        "\n",
        "print(f\"Random Forest (Business Class) Train MAPE: {train_mape_rf_b:.2f}%\")\n",
        "print(f\"Random Forest (Business Class) Test MAPE: {test_mape_rf_b:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb7e9fca",
        "outputId": "10fd53d7-0b6d-405f-e8dd-6963d46d27a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost (Economy Class) Train MAPE: 6.16%\n",
            "XGBoost (Economy Class) Test MAPE: 10.33%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_mape_xgb_e = mape(ye_full, pred_train_e)\n",
        "test_mape_xgb_e = mape(ye_test, pred_test_e)\n",
        "\n",
        "print(f\"XGBoost (Economy Class) Train MAPE: {train_mape_xgb_e:.2f}%\")\n",
        "print(f\"XGBoost (Economy Class) Test MAPE: {test_mape_xgb_e:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "kdW73OZAJwm9",
        "outputId": "be38ab3d-bcfc-4886-d1a1-f457781ae953"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Prepare your data (removing '(Test)' from columns)\n",
        "data = {\n",
        "    \"Model\": [\"Single Global Model\", \"Business Class Model\", \"Economy Class Model\"],\n",
        "    \"Algorithm\": [\"CatBoost\", \"Random Forest\", \"XGBoost\"],\n",
        "    \"R2 Score\": [0.980, 0.874, 0.837],\n",
        "    \"MAPE\": [\"11.93%\", \"4.53%\", \"10.33%\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Create the plot\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax.axis('off')  # Hide axes\n",
        "\n",
        "# 3. Create the table object\n",
        "the_table = ax.table(\n",
        "    cellText=df.values,\n",
        "    colLabels=df.columns,\n",
        "    cellLoc='center',\n",
        "    loc='center',\n",
        "    colColours=[\"#2d325a\"] * len(df.columns) # Dark blue header\n",
        ")\n",
        "\n",
        "# 4. Styling the table\n",
        "the_table.auto_set_font_size(False)\n",
        "the_table.set_fontsize(14)\n",
        "the_table.scale(1.2, 3)  # Adjust height and width\n",
        "\n",
        "# Style cells\n",
        "for (row, col), cell in the_table.get_celld().items():\n",
        "    # Header styling\n",
        "    if row == 0:\n",
        "        cell.get_text().set_color('white')\n",
        "        cell.get_text().set_weight('bold')\n",
        "    # Row colors (alternating for readability)\n",
        "    elif row % 2 == 0:\n",
        "        cell.set_facecolor('#f2f2f2')\n",
        "    else:\n",
        "        cell.set_facecolor('white')\n",
        "\n",
        "plt.title(\"Model Performance Comparison\", fontsize=18, pad=20, weight='bold')\n",
        "\n",
        "# 5. Save and Show\n",
        "plt.savefig('pretty_table.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "dc711a882efb41829e0ddba150876220",
            "c0a3042df72841158f0898fdb6570c40",
            "b209bc6d1d19442bb16f8b5815bcdd39",
            "df3e72e502ff44458e4ac298ac686c14",
            "5ec4275b7b014b51b0fc95a71e0d2f1e",
            "53a1723cb3504f5d85bc8eac5f0cfa03",
            "4ab33e9df551414988d060e9b2010558",
            "da344bc84ef54bb6a952408005528796",
            "8a693a0628d1486e9c1883ad11add3e7",
            "ea86bc4228fc44cd9c2ecace747155ba",
            "5ce82a929a544d6aa5f863e19c071d1f"
          ]
        },
        "id": "52YJp55RS-qQ",
        "outputId": "3962cf7d-0cf7-4099-a8f1-c0c2e5a178a1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc711a882efb41829e0ddba150876220",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Classes:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== MLR RESULTS (per class | same split) =====\n",
            "   class                 model  n_train  n_test  train_RMSE   train_MAE  train_R2   test_RMSE    test_MAE  test_R2\n",
            "Business MLR(LinearRegression)    24917    6229 8098.521424 5915.855870  0.608432 8007.135992 5905.482255 0.601544\n",
            " Economy MLR(LinearRegression)    55083   13771 2487.230144 1650.870318  0.556505 2478.597682 1645.820626 0.557069\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# MLR (Multiple Linear Regression) per class\n",
        "# - SAME train/test split via train_idx/test_idx (fair comparison)\n",
        "# - Separate model per class (Business / Economy)\n",
        "# - Drop flight\n",
        "# - log1p target transform, expm1 back to price for metrics\n",
        "# - OneHotEncode categoricals + impute missing values\n",
        "#\n",
        "# Assumes df is already loaded\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred),\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"R2\": float(r2_score(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def add_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "\n",
        "    X = add_interactions(X)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# SAME split indices (reuse if already created)\n",
        "# ------------------------------------------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Fit MLR per class\n",
        "# ------------------------------------------------------------\n",
        "rows = []\n",
        "models = {}\n",
        "\n",
        "for cls in tqdm(sorted(df[\"class\"].dropna().unique()), desc=\"Classes\"):\n",
        "    train_sub = df_train[df_train[\"class\"] == cls].copy()\n",
        "    test_sub  = df_test[df_test[\"class\"] == cls].copy()\n",
        "\n",
        "    X_train, y_train, y_train_log = prepare_X_y(train_sub)\n",
        "    X_test,  y_test,  _           = prepare_X_y(test_sub)\n",
        "\n",
        "    # align\n",
        "    X_test = X_test.reindex(columns=X_train.columns)\n",
        "\n",
        "    pre = build_preprocessor(X_train)\n",
        "\n",
        "    mlr = LinearRegression(n_jobs=-1)  # plain multiple linear regression\n",
        "    pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", mlr)])\n",
        "\n",
        "    pipe.fit(X_train, y_train_log)\n",
        "\n",
        "    pred_train = np.clip(np.expm1(pipe.predict(X_train)), 0, None)\n",
        "    pred_test  = np.clip(np.expm1(pipe.predict(X_test)),  0, None)\n",
        "\n",
        "    mt = compute_metrics(y_train, pred_train)\n",
        "    ms = compute_metrics(y_test, pred_test)\n",
        "\n",
        "    models[cls] = {\"pipe\": pipe, \"features\": X_train.columns.tolist()}\n",
        "\n",
        "    rows.append({\n",
        "        \"class\": cls,\n",
        "        \"model\": \"MLR(LinearRegression)\",\n",
        "        \"n_train\": len(X_train),\n",
        "        \"n_test\": len(X_test),\n",
        "        \"train_RMSE\": mt[\"RMSE\"],\n",
        "        \"train_MAE\":  mt[\"MAE\"],\n",
        "        \"train_R2\":   mt[\"R2\"],\n",
        "        \"test_RMSE\":  ms[\"RMSE\"],\n",
        "        \"test_MAE\":   ms[\"MAE\"],\n",
        "        \"test_R2\":    ms[\"R2\"],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows).sort_values(\"class\")\n",
        "print(\"\\n===== MLR RESULTS (per class | same split) =====\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Predict helper for NEW DATA\n",
        "# ------------------------------------------------------------\n",
        "def predict_price_mlr_by_class(new_df: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Predict price for new rows using the correct MLR model based on new_df['class'].\n",
        "    Returns predictions on ORIGINAL price scale.\n",
        "    \"\"\"\n",
        "    if \"class\" not in new_df.columns:\n",
        "        raise ValueError(\"new_df must contain a 'class' column (Business/Economy).\")\n",
        "\n",
        "    preds = np.full(len(new_df), np.nan, dtype=float)\n",
        "\n",
        "    for cls in models.keys():\n",
        "        mask = (new_df[\"class\"] == cls).values\n",
        "        if not mask.any():\n",
        "            continue\n",
        "\n",
        "        sub = new_df.loc[mask].copy()\n",
        "\n",
        "        if \"flight\" in sub.columns:\n",
        "            sub = sub.drop(columns=[\"flight\"])\n",
        "        if \"price\" in sub.columns:\n",
        "            sub = sub.drop(columns=[\"price\"])\n",
        "\n",
        "        sub = add_interactions(sub)\n",
        "        sub = sub.reindex(columns=models[cls][\"features\"])\n",
        "\n",
        "        log_pred = models[cls][\"pipe\"].predict(sub)\n",
        "        preds[mask] = np.clip(np.expm1(log_pred), 0, None)\n",
        "\n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw08WoC8ldPh"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FEATURE IMPORTANCE for ALL 3 MODEL TYPES:\n",
        "#   1) RandomForest (tree importance + permutation importance)\n",
        "#   2) XGBoost(native booster) (gain importance + permutation importance)\n",
        "#   3) MLR / LinearRegression (top coefficients + permutation importance)\n",
        "#\n",
        "# Works with your per-class workflow.\n",
        "# Assumes you ALREADY trained models and still have:\n",
        "#   - Business RF pipeline: rf_pipe  (Pipeline: pre + RandomForest)\n",
        "#   - Economy XGBoost objects: pre_e (ColumnTransformer), booster (xgb.Booster), best_it (int)\n",
        "#   - MLR pipelines per class in dict: mlr_models[cls] = {\"pipe\": Pipeline, \"features\": [...]}\n",
        "# Also assumes you still have df_train / df_test and same splits.\n",
        "#\n",
        "# If your variable names differ, replace them in the \"REQUIRED OBJECTS\" section.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "from scipy import sparse\n",
        "import xgboost as xgb\n",
        "\n",
        "# ----------------------------\n",
        "# Utility helpers\n",
        "# ----------------------------\n",
        "def to_csr(X):\n",
        "    return X.tocsr() if sparse.issparse(X) else X\n",
        "\n",
        "def get_ohe_feature_names(preprocessor):\n",
        "    \"\"\"\n",
        "    Returns expanded feature names after ColumnTransformer with OneHotEncoder inside.\n",
        "    Works for the preprocessors we built earlier.\n",
        "    \"\"\"\n",
        "    feat_names = []\n",
        "    for name, trans, cols in preprocessor.transformers_:\n",
        "        if name == \"remainder\" and trans == \"drop\":\n",
        "            continue\n",
        "        if hasattr(trans, \"named_steps\") and \"onehot\" in trans.named_steps:\n",
        "            ohe = trans.named_steps[\"onehot\"]\n",
        "            # cols is list of original categorical column names\n",
        "            ohe_names = list(ohe.get_feature_names_out(cols))\n",
        "            feat_names.extend(ohe_names)\n",
        "        else:\n",
        "            # numeric pipeline: keep original numeric column names\n",
        "            if isinstance(cols, (list, tuple, np.ndarray, pd.Index)):\n",
        "                feat_names.extend(list(cols))\n",
        "            else:\n",
        "                feat_names.append(str(cols))\n",
        "    return np.array(feat_names, dtype=object)\n",
        "\n",
        "def top_n_series(s: pd.Series, n=25):\n",
        "    return s.sort_values(ascending=False).head(n)\n",
        "\n",
        "def print_top(title, s: pd.Series, n=25):\n",
        "    print(f\"\\n=== {title} (top {n}) ===\")\n",
        "    print(top_n_series(s, n).to_string())\n",
        "\n",
        "# ----------------------------\n",
        "# REQUIRED OBJECTS (edit if needed)\n",
        "# ----------------------------\n",
        "# Business:\n",
        "#   rf_pipe  -> Pipeline(pre, rf)\n",
        "#   Xb_train, Xb_test, yb_train, yb_test -> (optional, for permutation on test)\n",
        "# Economy:\n",
        "#   pre_e, booster, best_it, Xe_test_df, ye_test -> for permutation on test\n",
        "# MLR:\n",
        "#   You should have something like:\n",
        "#     mlr_models = {\"Business\": {\"pipe\": pipe_b, \"features\": [...]}, \"Economy\": {...}}\n",
        "# If you don't have mlr_models dict, you can still run coefficient extraction on any LinearRegression pipeline you have.\n",
        "\n",
        "# ============================================================\n",
        "# 1) RANDOM FOREST FEATURE IMPORTANCE (Business)\n",
        "# ============================================================\n",
        "def rf_feature_importance(rf_pipe, X_test, y_test, top_n=25, n_repeats=5):\n",
        "    pre = rf_pipe.named_steps[\"pre\"]\n",
        "    rf = rf_pipe.named_steps[\"model\"]\n",
        "\n",
        "    # Built-in impurity importance (fast)\n",
        "    ohe_names = get_ohe_feature_names(pre)\n",
        "    imp = pd.Series(rf.feature_importances_, index=ohe_names)\n",
        "    print_top(\"RandomForest built-in importance (impurity)\", imp, top_n)\n",
        "\n",
        "    # Permutation importance on TEST (more trustworthy)\n",
        "    # Note: permutation_importance works on the whole pipeline directly\n",
        "    perm = permutation_importance(\n",
        "        rf_pipe, X_test, np.log1p(y_test),  # model is trained on log target\n",
        "        scoring=\"neg_mean_absolute_error\",\n",
        "        n_repeats=n_repeats,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    perm_imp = pd.Series(perm.importances_mean, index=get_ohe_feature_names(pre))\n",
        "    print_top(\"RandomForest permutation importance (TEST, MAE)\", perm_imp, top_n)\n",
        "\n",
        "# ============================================================\n",
        "# 2) XGBOOST FEATURE IMPORTANCE (Economy)\n",
        "# ============================================================\n",
        "def xgb_feature_importance(pre_e, booster, best_it, X_test_df, y_test, top_n=25, n_repeats=5):\n",
        "    # Built-in booster importance: use \"gain\" (most meaningful)\n",
        "    score_gain = booster.get_score(importance_type=\"gain\")\n",
        "    gain = pd.Series(score_gain).sort_values(ascending=False)\n",
        "\n",
        "    # Map f0,f1,... to actual one-hot feature names\n",
        "    feat_names = get_ohe_feature_names(pre_e)\n",
        "    def map_f_to_name(k):\n",
        "        if k.startswith(\"f\"):\n",
        "            idx = int(k[1:])\n",
        "            if 0 <= idx < len(feat_names):\n",
        "                return feat_names[idx]\n",
        "        return k\n",
        "\n",
        "    gain.index = [map_f_to_name(k) for k in gain.index]\n",
        "    print_top(\"XGBoost built-in importance (GAIN)\", gain, top_n)\n",
        "\n",
        "    # Permutation importance on TEST (using custom predict wrapper)\n",
        "    # We create a small wrapper object with predict() so sklearn can permute X_test_df columns.\n",
        "    class XGBWrapper:\n",
        "        def __init__(self, pre, booster, best_it):\n",
        "            self.pre = pre\n",
        "            self.booster = booster\n",
        "            self.best_it = int(best_it)\n",
        "\n",
        "        def predict(self, X):\n",
        "            Xm = to_csr(self.pre.transform(X))\n",
        "            d = xgb.DMatrix(Xm)\n",
        "            # model predicts log(price)\n",
        "            return self.booster.predict(d, iteration_range=(0, self.best_it + 1))\n",
        "\n",
        "    xw = XGBWrapper(pre_e, booster, best_it)\n",
        "\n",
        "    perm = permutation_importance(\n",
        "        xw, X_test_df, np.log1p(y_test),\n",
        "        scoring=\"neg_mean_absolute_error\",\n",
        "        n_repeats=n_repeats,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Here permutation_importance returns importances per ORIGINAL columns (not one-hot).\n",
        "    perm_imp = pd.Series(perm.importances_mean, index=X_test_df.columns).sort_values(ascending=False)\n",
        "    print_top(\"XGBoost permutation importance (TEST, MAE) [original columns]\", perm_imp, top_n)\n",
        "\n",
        "# ============================================================\n",
        "# 3) MLR FEATURE IMPORTANCE (coefficients + permutation)\n",
        "# ============================================================\n",
        "def mlr_feature_importance(mlr_pipe, X_test, y_test, top_n=25, n_repeats=5):\n",
        "    pre = mlr_pipe.named_steps[\"pre\"]\n",
        "    lr = mlr_pipe.named_steps[\"model\"]\n",
        "\n",
        "    # Coefficients are in log-space because training target = log1p(price)\n",
        "    names = get_ohe_feature_names(pre)\n",
        "    coefs = pd.Series(np.abs(lr.coef_), index=names).sort_values(ascending=False)\n",
        "    print_top(\"MLR coefficient importance (abs(coef), log-space)\", coefs, top_n)\n",
        "\n",
        "    # Permutation importance on TEST\n",
        "    perm = permutation_importance(\n",
        "        mlr_pipe, X_test, np.log1p(y_test),\n",
        "        scoring=\"neg_mean_absolute_error\",\n",
        "        n_repeats=n_repeats,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    perm_imp = pd.Series(perm.importances_mean, index=get_ohe_feature_names(pre)).sort_values(ascending=False)\n",
        "    print_top(\"MLR permutation importance (TEST, MAE)\", perm_imp, top_n)\n",
        "\n",
        "# ============================================================\n",
        "# RUN SECTION\n",
        "# ============================================================\n",
        "# ---- RandomForest (Business) ----\n",
        "# You need Xb_test and yb_test from your Business prep step\n",
        "# rf_feature_importance(rf_pipe, Xb_test, yb_test, top_n=30, n_repeats=5)\n",
        "\n",
        "# ---- XGBoost (Economy) ----\n",
        "# You need Xe_test_df and ye_test from your Economy prep step\n",
        "# xgb_feature_importance(pre_e, booster, best_it, Xe_test_df, ye_test, top_n=30, n_repeats=5)\n",
        "\n",
        "# ---- MLR (both classes) ----\n",
        "# Example if you stored them in a dict:\n",
        "# for cls in [\"Business\", \"Economy\"]:\n",
        "#     mlr_feature_importance(mlr_models[cls][\"pipe\"], X_test_by_class[cls], y_test_by_class[cls], top_n=30, n_repeats=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "SzyefUvsA1Du",
        "outputId": "e721ed48-bc03-4f70-cf16-7ca42a3ad7e6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'rf_pipe' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3819937526.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_feature_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'rf_pipe' is not defined"
          ]
        }
      ],
      "source": [
        "rf_feature_importance(rf_pipe, Xb_test, yb_test, top_n=30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNFgvzpaBOQv",
        "outputId": "5d3f42a9-2eb7-4798-c1a7-cbca977d67d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== RandomForest built-in importance (top 30) ===\n",
            "duration                                   0.559481\n",
            "stops_zero                                 0.078847\n",
            "days_left                                  0.040374\n",
            "airline_Air_India                          0.036283\n",
            "airline_Vistara                            0.032987\n",
            "stops_one                                  0.021341\n",
            "source_city_Delhi                          0.012186\n",
            "destination_city_Delhi                     0.011027\n",
            "arrival_time_Evening                       0.009880\n",
            "source_city_Kolkata                        0.008510\n",
            "destination_city_Kolkata                   0.007379\n",
            "arrival_time_Afternoon                     0.007262\n",
            "arrival_time_Night                         0.007134\n",
            "departure_time_Evening                     0.006860\n",
            "destination_city_Hyderabad                 0.006579\n",
            "arrival_time_Early_Morning                 0.005545\n",
            "departure_time_Afternoon                   0.005311\n",
            "departure_time_Morning                     0.005223\n",
            "destination_city_Mumbai                    0.005004\n",
            "departure_time_Early_Morning               0.004667\n",
            "departure_time_Night                       0.004567\n",
            "source_city_Hyderabad                      0.004394\n",
            "arrival_time_Morning                       0.003980\n",
            "source_city_Mumbai                         0.003752\n",
            "stops_two_or_more                          0.003703\n",
            "destination_city_Bangalore                 0.003465\n",
            "route_Bangalore->Mumbai                    0.003390\n",
            "route_Mumbai->Bangalore                    0.003353\n",
            "route_airline_Vistara_Mumbai->Bangalore    0.003269\n",
            "route_Delhi->Mumbai                        0.003169\n",
            "\n",
            "=== RandomForest permutation importance on TEST (top 30) ===\n",
            "duration            0.139849\n",
            "airline             0.067679\n",
            "destination_city    0.038521\n",
            "source_city         0.033809\n",
            "arrival_time        0.027304\n",
            "departure_time      0.024550\n",
            "route               0.023076\n",
            "route_airline       0.020856\n",
            "stops               0.013072\n",
            "days_left           0.010720\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# -----------------------------\n",
        "# Settings (Business RF compromise)\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "BUSINESS_RF = dict(\n",
        "    n_estimators=800,\n",
        "    max_depth=40,\n",
        "    min_samples_leaf=2,\n",
        "    min_samples_split=8,\n",
        "    max_features=0.8,\n",
        "    bootstrap=True,\n",
        "    max_samples=0.9,\n",
        "    n_jobs=-1,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def add_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def prepare_X_y(df_sub):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "    X = add_interactions(X)\n",
        "    return X, y, y_log\n",
        "\n",
        "def build_preprocessor(X):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
        "    cat_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        [(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "\n",
        "def get_ohe_feature_names(preprocessor):\n",
        "    names = []\n",
        "    for name, trans, cols in preprocessor.transformers_:\n",
        "        if name == \"remainder\" and trans == \"drop\":\n",
        "            continue\n",
        "        if hasattr(trans, \"named_steps\") and \"onehot\" in trans.named_steps:\n",
        "            ohe = trans.named_steps[\"onehot\"]\n",
        "            names.extend(list(ohe.get_feature_names_out(cols)))\n",
        "        else:\n",
        "            names.extend(list(cols))\n",
        "    return np.array(names, dtype=object)\n",
        "\n",
        "def rf_feature_importance(rf_pipe, X_test, y_test, top_n=30, n_repeats=5):\n",
        "    pre = rf_pipe.named_steps[\"pre\"]\n",
        "    rf = rf_pipe.named_steps[\"model\"]\n",
        "\n",
        "    # 1) built-in impurity importance\n",
        "    # This is for the internal features of the model after preprocessing\n",
        "    ohe_feat_names = get_ohe_feature_names(pre)\n",
        "    imp = pd.Series(rf.feature_importances_, index=ohe_feat_names).sort_values(ascending=False)\n",
        "    print(f\"\\n=== RandomForest built-in importance (top {top_n}) ===\")\n",
        "    print(imp.head(top_n).to_string())\n",
        "\n",
        "    # 2) permutation importance on TEST (on log scale because model predicts log1p(price))\n",
        "    # As per sklearn documentation, when a Pipeline with a preprocessor is passed,\n",
        "    # permutation_importance computes importance on the ORIGINAL features of X_test.\n",
        "    perm = permutation_importance(\n",
        "        rf_pipe,\n",
        "        X_test,\n",
        "        np.log1p(y_test),\n",
        "        scoring=\"neg_mean_absolute_error\",\n",
        "        n_repeats=n_repeats,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    # The importances_mean here will correspond to the ORIGINAL columns of X_test\n",
        "    perm_imp = pd.Series(perm.importances_mean, index=X_test.columns).sort_values(ascending=False)\n",
        "    print(f\"\\n=== RandomForest permutation importance on TEST (top {top_n}) ===\")\n",
        "    print(perm_imp.head(top_n).to_string())\n",
        "\n",
        "# -----------------------------\n",
        "# (Re)create split indices if missing\n",
        "# -----------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -----------------------------\n",
        "# Train Business RF pipeline (defines rf_pipe, Xb_test, yb_test)\n",
        "# -----------------------------\n",
        "train_b = df_train[df_train[\"class\"] == \"Business\"].copy()\n",
        "test_b  = df_test[df_test[\"class\"] == \"Business\"].copy()\n",
        "\n",
        "Xb_train, yb_train, yb_train_log = prepare_X_y(train_b)\n",
        "Xb_test,  yb_test,  _            = prepare_X_y(test_b)\n",
        "\n",
        "# align columns\n",
        "Xb_test = Xb_test.reindex(columns=Xb_train.columns)\n",
        "\n",
        "pre_b = build_preprocessor(Xb_train)\n",
        "\n",
        "rf_pipe = Pipeline([\n",
        "    (\"pre\", pre_b),\n",
        "    (\"model\", RandomForestRegressor(**BUSINESS_RF))\n",
        "])\n",
        "\n",
        "rf_pipe.fit(Xb_train, yb_train_log)\n",
        "\n",
        "# -----------------------------\n",
        "# Now this will work (no NameError)\n",
        "# -----------------------------\n",
        "rf_feature_importance(rf_pipe, Xb_test, yb_test, top_n=30, n_repeats=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "IE86D6qMCSox",
        "outputId": "0266fe85-ee10-4f73-8083-942fa65c0d26"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pre_e' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3735494285.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m# Built-in (GAIN) importance (one-hot expanded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0mxgb_gain_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Permutation importance (original columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pre_e' is not defined"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# XGBoost (Economy) FEATURE IMPORTANCE\n",
        "# 1) Built-in importance (GAIN) from the booster\n",
        "# 2) Permutation importance on TEST (more trustworthy)\n",
        "#\n",
        "# Assumes you ALREADY trained Economy XGBoost and still have:\n",
        "#   - pre_e        : ColumnTransformer used for Economy (fit)\n",
        "#   - booster      : xgb.Booster\n",
        "#   - best_it      : int (best_iteration)\n",
        "#   - Xe_test_df   : Economy test DataFrame (original feature columns, after your feature engineering)\n",
        "#   - ye_test      : Economy test prices on ORIGINAL scale\n",
        "#\n",
        "# If any of these names differ in your notebook, replace accordingly.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.inspection import permutation_importance\n",
        "from scipy import sparse\n",
        "import xgboost as xgb\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def to_csr(X):\n",
        "    return X.tocsr() if sparse.issparse(X) else X\n",
        "\n",
        "def get_ohe_feature_names(preprocessor):\n",
        "    names = []\n",
        "    for name, trans, cols in preprocessor.transformers_:\n",
        "        if name == \"remainder\" and trans == \"drop\":\n",
        "            continue\n",
        "        if hasattr(trans, \"named_steps\") and \"onehot\" in trans.named_steps:\n",
        "            ohe = trans.named_steps[\"onehot\"]\n",
        "            names.extend(list(ohe.get_feature_names_out(cols)))\n",
        "        else:\n",
        "            names.extend(list(cols))\n",
        "    return np.array(names, dtype=object)\n",
        "\n",
        "def xgb_gain_importance(pre_e, booster, top_n=30):\n",
        "    \"\"\"\n",
        "    Built-in XGBoost importance using GAIN, mapped to readable feature names.\n",
        "    \"\"\"\n",
        "    feat_names = get_ohe_feature_names(pre_e)\n",
        "\n",
        "    gain_dict = booster.get_score(importance_type=\"gain\")\n",
        "    if not gain_dict:\n",
        "        print(\"No gain importance found. (Model might not have splits or importance unavailable.)\")\n",
        "        return\n",
        "\n",
        "    gain = pd.Series(gain_dict).sort_values(ascending=False)\n",
        "\n",
        "    def map_f_to_name(k):\n",
        "        # keys look like 'f0','f1',...\n",
        "        if k.startswith(\"f\"):\n",
        "            i = int(k[1:])\n",
        "            if 0 <= i < len(feat_names):\n",
        "                return feat_names[i]\n",
        "        return k\n",
        "\n",
        "    gain.index = [map_f_to_name(k) for k in gain.index]\n",
        "\n",
        "    print(f\"\\n=== XGBoost built-in feature importance (GAIN) top {top_n} ===\")\n",
        "    print(gain.head(top_n).to_string())\n",
        "\n",
        "class XGBWrapper:\n",
        "    \"\"\"\n",
        "    Minimal wrapper so sklearn.permutation_importance can call .predict(X_df)\n",
        "    and we internally apply preprocessor + xgb predict.\n",
        "    \"\"\"\n",
        "    def __init__(self, pre, booster, best_it):\n",
        "        self.pre = pre\n",
        "        self.booster = booster\n",
        "        self.best_it = int(best_it)\n",
        "\n",
        "    def predict(self, X_df):\n",
        "        Xm = to_csr(self.pre.transform(X_df))\n",
        "        d = xgb.DMatrix(Xm)\n",
        "        # model outputs log1p(price)\n",
        "        return self.booster.predict(d, iteration_range=(0, self.best_it + 1))\n",
        "\n",
        "def xgb_permutation_importance(pre_e, booster, best_it, Xe_test_df, ye_test, top_n=30, n_repeats=5):\n",
        "    \"\"\"\n",
        "    Permutation importance on TEST, measured on log1p(price) target,\n",
        "    returned for ORIGINAL (pre-onehot) columns (easier to interpret).\n",
        "    \"\"\"\n",
        "    model = XGBWrapper(pre_e, booster, best_it)\n",
        "\n",
        "    perm = permutation_importance(\n",
        "        model,\n",
        "        Xe_test_df,\n",
        "        np.log1p(ye_test),  # log target because model predicts log1p(price)\n",
        "        scoring=\"neg_mean_absolute_error\",\n",
        "        n_repeats=n_repeats,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    perm_imp = pd.Series(perm.importances_mean, index=Xe_test_df.columns).sort_values(ascending=False)\n",
        "    print(f\"\\n=== XGBoost permutation importance on TEST (MAE) top {top_n} ===\")\n",
        "    print(perm_imp.head(top_n).to_string())\n",
        "\n",
        "# ----------------------------\n",
        "# RUN\n",
        "# ----------------------------\n",
        "# Built-in (GAIN) importance (one-hot expanded)\n",
        "xgb_gain_importance(pre_e, booster, top_n=30)\n",
        "\n",
        "# Permutation importance (original columns)\n",
        "xgb_permutation_importance(pre_e, booster, best_it, Xe_test_df, ye_test, top_n=30, n_repeats=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7jH3GEtrMt3e",
        "outputId": "59ca8b55-f186-4883-b010-fc88ca562309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Economy -> XGBoost(native) with early stopping ...\n",
            "[0]\tval-rmse:0.50349\n",
            "[200]\tval-rmse:0.19345\n",
            "[400]\tval-rmse:0.18339\n",
            "[600]\tval-rmse:0.17689\n",
            "[800]\tval-rmse:0.17348\n",
            "[1000]\tval-rmse:0.17113\n",
            "[1200]\tval-rmse:0.16962\n",
            "[1400]\tval-rmse:0.16868\n",
            "[1600]\tval-rmse:0.16774\n",
            "[1800]\tval-rmse:0.16732\n",
            "[2000]\tval-rmse:0.16685\n",
            "[2200]\tval-rmse:0.16661\n",
            "[2400]\tval-rmse:0.16631\n",
            "[2600]\tval-rmse:0.16618\n",
            "[2800]\tval-rmse:0.16606\n",
            "[2994]\tval-rmse:0.16614\n",
            "\n",
            "Best iteration (Economy XGB): 2794\n",
            "\n",
            "=== XGBoost built-in feature importance (GAIN) top 30 ===\n",
            "airline_AirAsia                                4.347416\n",
            "airline_Indigo                                 1.266701\n",
            "source_city_Kolkata                            1.173146\n",
            "route_Mumbai->Hyderabad                        1.170618\n",
            "route_Delhi->Bangalore                         0.940783\n",
            "airline_Vistara                                0.885816\n",
            "route_Chennai->Hyderabad                       0.841112\n",
            "route_Bangalore->Delhi                         0.834327\n",
            "route_Hyderabad->Mumbai                        0.780083\n",
            "destination_city_Kolkata                       0.777184\n",
            "stops_two_or_more                              0.736788\n",
            "airline_Air_India                              0.678237\n",
            "airline_GO_FIRST                               0.544635\n",
            "route_airline_Indigo_Hyderabad->Bangalore      0.514091\n",
            "route_Hyderabad->Bangalore                     0.476018\n",
            "stops_zero                                     0.438395\n",
            "route_airline_Indigo_Bangalore->Hyderabad      0.437183\n",
            "route_Bangalore->Hyderabad                     0.416011\n",
            "route_Mumbai->Bangalore                        0.390285\n",
            "route_Mumbai->Chennai                          0.380878\n",
            "route_Bangalore->Mumbai                        0.370678\n",
            "route_Chennai->Mumbai                          0.354322\n",
            "days_left                                      0.344391\n",
            "route_airline_SpiceJet_Bangalore->Hyderabad    0.342847\n",
            "airline_SpiceJet                               0.331365\n",
            "route_airline_Vistara_Kolkata->Chennai         0.331361\n",
            "destination_city_Chennai                       0.328244\n",
            "route_Mumbai->Kolkata                          0.327017\n",
            "route_Delhi->Chennai                           0.317926\n",
            "source_city_Delhi                              0.300038\n"
          ]
        },
        {
          "ename": "InvalidParameterError",
          "evalue": "The 'estimator' parameter of permutation_importance must be an object implementing 'fit'. Got <__main__.XGBWrapper object at 0x7a0f478996a0> instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2361015320.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0mxw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m perm = permutation_importance(\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0mxw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mXe_test_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_ignore\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             validate_parameter_constraints(\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0mparameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 )\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise InvalidParameterError(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\"The {param_name!r} parameter of {caller_name} must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34mf\" {constraints_str}. Got {param_val!r} instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidParameterError\u001b[0m: The 'estimator' parameter of permutation_importance must be an object implementing 'fit'. Got <__main__.XGBWrapper object at 0x7a0f478996a0> instead."
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# FIX for NameError: pre_e not defined\n",
        "# -> Rebuild Economy XGBoost (native early stopping) + Feature Importance\n",
        "#\n",
        "# Outputs:\n",
        "# 1) XGBoost built-in feature importance (GAIN) [one-hot expanded]\n",
        "# 2) Permutation importance on TEST (MAE) [original columns]\n",
        "#\n",
        "# Assumes df is already loaded.\n",
        "# Uses SAME train/test split indices if train_idx/test_idx exist; otherwise creates them.\n",
        "# Drops flight, uses log1p target, expm1 back only for metrics (importance uses log-space).\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "import xgboost as xgb\n",
        "from scipy import sparse\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "# ---- XGBoost params (as-is style) ----\n",
        "XGB_PARAMS = {\n",
        "    \"eta\": 0.05,\n",
        "    \"max_depth\": 10,\n",
        "    \"min_child_weight\": 5,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"lambda\": 2.0,\n",
        "    \"alpha\": 0.0,\n",
        "    \"gamma\": 0.0,\n",
        "}\n",
        "XGB_TREE_METHOD = \"hist\"\n",
        "XGB_NUM_BOOST_ROUND = 20000\n",
        "XGB_EARLY_STOPPING_ROUNDS = 200\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def to_csr(X):\n",
        "    return X.tocsr() if sparse.issparse(X) else X\n",
        "\n",
        "def make_split_indices(df):\n",
        "    idx = np.arange(len(df))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx, test_size=TEST_SIZE, random_state=SEED, shuffle=True, stratify=df[\"class\"]\n",
        "    )\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def add_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if \"source_city\" in X.columns and \"destination_city\" in X.columns:\n",
        "        X[\"route\"] = X[\"source_city\"].astype(str) + \"->\" + X[\"destination_city\"].astype(str)\n",
        "    if \"airline\" in X.columns and \"route\" in X.columns:\n",
        "        X[\"route_airline\"] = X[\"airline\"].astype(str) + \"_\" + X[\"route\"].astype(str)\n",
        "    return X\n",
        "\n",
        "def prepare_X_y(df_sub: pd.DataFrame):\n",
        "    df_sub = df_sub.copy()\n",
        "    df_sub[\"price\"] = pd.to_numeric(df_sub[\"price\"], errors=\"coerce\")\n",
        "    df_sub = df_sub.dropna(subset=[\"price\"])\n",
        "\n",
        "    y = df_sub[\"price\"].astype(float).values\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    drop_cols = [c for c in [\"price\", \"class\", \"flight\"] if c in df_sub.columns]\n",
        "    X = df_sub.drop(columns=drop_cols)\n",
        "    X = add_interactions(X)\n",
        "    return X, y, y_log\n",
        "\n",
        "def get_feature_lists(X: pd.DataFrame):\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "    return num_cols, cat_cols\n",
        "\n",
        "def make_onehot_preprocessor(num_cols, cat_cols):\n",
        "    num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
        "    cat_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "\n",
        "def get_ohe_feature_names(preprocessor):\n",
        "    names = []\n",
        "    for name, trans, cols in preprocessor.transformers_:\n",
        "        if name == \"remainder\" and trans == \"drop\":\n",
        "            continue\n",
        "        if hasattr(trans, \"named_steps\") and \"onehot\" in trans.named_steps:\n",
        "            ohe = trans.named_steps[\"onehot\"]\n",
        "            names.extend(list(ohe.get_feature_names_out(cols)))\n",
        "        else:\n",
        "            names.extend(list(cols))\n",
        "    return np.array(names, dtype=object)\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Same split indices\n",
        "# -----------------------------\n",
        "if \"train_idx\" not in globals() or \"test_idx\" not in globals():\n",
        "    train_idx, test_idx = make_split_indices(df)\n",
        "\n",
        "df_train = df.iloc[train_idx].copy()\n",
        "df_test  = df.iloc[test_idx].copy()\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Build Economy XGBoost (defines pre_e, booster, best_it, Xe_test_df, ye_test)\n",
        "# -----------------------------\n",
        "train_e = df_train[df_train[\"class\"] == \"Economy\"].copy()\n",
        "test_e  = df_test[df_test[\"class\"] == \"Economy\"].copy()\n",
        "\n",
        "Xe_full, ye_full, ye_full_log = prepare_X_y(train_e)\n",
        "Xe_test_df, ye_test, _        = prepare_X_y(test_e)\n",
        "\n",
        "# align (important)\n",
        "Xe_test_df = Xe_test_df.reindex(columns=Xe_full.columns)\n",
        "\n",
        "# internal train/val for early stopping\n",
        "Xe_tr_df, Xe_val_df, ye_tr, ye_val, ye_tr_log, ye_val_log = train_test_split(\n",
        "    Xe_full, ye_full, ye_full_log,\n",
        "    test_size=VAL_SIZE, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "num_cols_e, cat_cols_e = get_feature_lists(Xe_tr_df)\n",
        "pre_e = make_onehot_preprocessor(num_cols_e, cat_cols_e)\n",
        "\n",
        "Xe_tr  = to_csr(pre_e.fit_transform(Xe_tr_df))\n",
        "Xe_val = to_csr(pre_e.transform(Xe_val_df))\n",
        "Xe_full_mat = to_csr(pre_e.transform(Xe_full))\n",
        "Xe_test_mat = to_csr(pre_e.transform(Xe_test_df))\n",
        "\n",
        "feat_names = list(get_ohe_feature_names(pre_e))\n",
        "\n",
        "dtrain = xgb.DMatrix(Xe_tr, label=ye_tr_log, feature_names=feat_names)\n",
        "dval   = xgb.DMatrix(Xe_val, label=ye_val_log, feature_names=feat_names)\n",
        "dfull  = xgb.DMatrix(Xe_full_mat, label=ye_full_log, feature_names=feat_names)\n",
        "dtest  = xgb.DMatrix(Xe_test_mat, feature_names=feat_names)\n",
        "\n",
        "xgb_params = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"eval_metric\": \"rmse\",\n",
        "    \"tree_method\": XGB_TREE_METHOD,\n",
        "    \"seed\": SEED,\n",
        "    **XGB_PARAMS\n",
        "}\n",
        "\n",
        "print(\"\\nTraining Economy -> XGBoost(native) with early stopping ...\")\n",
        "booster = xgb.train(\n",
        "    params=xgb_params,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=XGB_NUM_BOOST_ROUND,\n",
        "    evals=[(dval, \"val\")],\n",
        "    early_stopping_rounds=XGB_EARLY_STOPPING_ROUNDS,\n",
        "    verbose_eval=200\n",
        ")\n",
        "\n",
        "best_it = int(booster.best_iteration)\n",
        "print(f\"\\nBest iteration (Economy XGB): {best_it}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Feature importance (GAIN) - one-hot expanded\n",
        "# -----------------------------\n",
        "gain = booster.get_score(importance_type=\"gain\")\n",
        "gain_s = pd.Series(gain).sort_values(ascending=False)\n",
        "\n",
        "top_n = 30\n",
        "print(f\"\\n=== XGBoost built-in feature importance (GAIN) top {top_n} ===\")\n",
        "print(gain_s.head(top_n).to_string())\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Permutation importance on TEST (original columns)\n",
        "#    Note: This permutes original DataFrame columns (NOT one-hot columns).\n",
        "#    Target is log1p(price) because model predicts log1p(price).\n",
        "# -----------------------------\n",
        "class XGBWrapper:\n",
        "    def __init__(self, pre, booster, best_it, feat_names):\n",
        "        self.pre = pre\n",
        "        self.booster = booster\n",
        "        self.best_it = int(best_it)\n",
        "        self.feat_names = feat_names\n",
        "\n",
        "    def predict(self, X_df):\n",
        "        Xm = to_csr(self.pre.transform(X_df))\n",
        "        d = xgb.DMatrix(Xm, feature_names=self.feat_names)\n",
        "        return self.booster.predict(d, iteration_range=(0, self.best_it + 1))\n",
        "\n",
        "xw = XGBWrapper(pre_e, booster, best_it, feat_names)\n",
        "\n",
        "perm = permutation_importance(\n",
        "    xw,\n",
        "    Xe_test_df,\n",
        "    np.log1p(ye_test),\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    n_repeats=5,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "perm_s = pd.Series(perm.importances_mean, index=Xe_test_df.columns).sort_values(ascending=False)\n",
        "\n",
        "print(f\"\\n=== XGBoost permutation importance on TEST (MAE) top {top_n} ===\")\n",
        "print(perm_s.head(top_n).to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCK9bhGNOXY6",
        "outputId": "1f9f569a-7e23-4f2c-863b-249d7bc8a5a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== XGBoost permutation importance on TEST (MAE) top 30 ===\n",
            "days_left           0.260703\n",
            "duration            0.137041\n",
            "airline             0.130266\n",
            "route_airline       0.050233\n",
            "route               0.036153\n",
            "source_city         0.035046\n",
            "departure_time      0.023513\n",
            "destination_city    0.023429\n",
            "arrival_time        0.022974\n",
            "stops               0.022326\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "import xgboost as xgb\n",
        "from scipy import sparse\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def to_csr(X):\n",
        "    return X.tocsr() if sparse.issparse(X) else X\n",
        "\n",
        "# ---- estimator wrapper that sklearn accepts (has fit + predict) ----\n",
        "class XGBSklearnWrapper(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, pre, booster, best_it, feature_names):\n",
        "        self.pre = pre\n",
        "        self.booster = booster\n",
        "        self.best_it = int(best_it)\n",
        "        self.feature_names = list(feature_names)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # no-op: model is already trained (booster)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        Xm = to_csr(self.pre.transform(X))\n",
        "        d = xgb.DMatrix(Xm, feature_names=self.feature_names)\n",
        "        return self.booster.predict(d, iteration_range=(0, self.best_it + 1))\n",
        "\n",
        "# ---- make sure these exist from your previous training cell ----\n",
        "# pre_e, booster, best_it, feat_names, Xe_test_df, ye_test\n",
        "\n",
        "xw = XGBSklearnWrapper(pre_e, booster, best_it, feat_names)\n",
        "\n",
        "perm = permutation_importance(\n",
        "    estimator=xw,\n",
        "    X=Xe_test_df,\n",
        "    y=np.log1p(ye_test),                 # because model predicts log1p(price)\n",
        "    scoring=\"neg_mean_absolute_error\",   # permutation effect measured on MAE\n",
        "    n_repeats=5,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "perm_s = pd.Series(perm.importances_mean, index=Xe_test_df.columns).sort_values(ascending=False)\n",
        "\n",
        "top_n = 30\n",
        "print(f\"\\n=== XGBoost permutation importance on TEST (MAE) top {top_n} ===\")\n",
        "print(perm_s.head(top_n).to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "83zB3m_6LNKu",
        "outputId": "f4117637-2a28-4437-cd25-86778a324811"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Data from your results\n",
        "data = {\n",
        "    'Feature': [\n",
        "        'Duration', 'Airline', 'Destination City', 'Source City',\n",
        "        'Arrival Time', 'Departure Time', 'Route', 'Route Airline',\n",
        "        'Stops', 'Days Left'\n",
        "    ],\n",
        "    'Importance': [\n",
        "        0.139849, 0.067679, 0.038521, 0.033809, 0.027304,\n",
        "        0.024550, 0.023076, 0.020856, 0.013072, 0.010720\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Convert Importance to Percentage\n",
        "df['Importance_Pct'] = df['Importance'] * 100\n",
        "df = df.sort_values(by='Importance_Pct', ascending=True)\n",
        "\n",
        "# 3. Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.barh(df['Feature'], df['Importance_Pct'], color='#27AE60', edgecolor='black')\n",
        "\n",
        "# 4. Add percentage labels at the end of each bar\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    plt.text(width + 0.3, bar.get_y() + bar.get_height()/2,\n",
        "             f'{width:.1f}%', va='center', fontweight='bold', color='#2c3e50')\n",
        "\n",
        "# 5. Styling\n",
        "plt.xlabel('Importance (%)', fontsize=12)\n",
        "plt.title('Feature Importance: Business Class', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "\n",
        "# Modern cleanup\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "# Increase the x-axis limit slightly to make room for the labels\n",
        "plt.xlim(0, max(df['Importance_Pct']) + 2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "jmCxI60qLgB4",
        "outputId": "3c98aeb7-e4e6-4e18-a92f-ee64e3dd88ae"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Data for Economy Class (Top 10)\n",
        "data_economy = {\n",
        "    'Feature': [\n",
        "        'Days Left', 'Duration', 'Airline', 'Route Airline',\n",
        "        'Route', 'Source City', 'Departure Time',\n",
        "        'Destination City', 'Arrival Time', 'Stops'\n",
        "    ],\n",
        "    'Importance': [\n",
        "        0.260703, 0.137041, 0.130266, 0.050233,\n",
        "        0.036153, 0.035046, 0.023513, 0.023429,\n",
        "        0.022974, 0.022326\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 2. Create DataFrame and convert Importance to Percentage\n",
        "df_econ = pd.DataFrame(data_economy)\n",
        "df_econ['Importance_Pct'] = df_econ['Importance'] * 100\n",
        "df_econ = df_econ.sort_values(by='Importance_Pct', ascending=True)\n",
        "\n",
        "# 3. Create the plot with Sea Green color\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.barh(df_econ['Feature'], df_econ['Importance_Pct'], color='#16a085', edgecolor='black')\n",
        "\n",
        "# 4. Add percentage labels at the end of each bar\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    plt.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "             f'{width:.1f}%', va='center', fontweight='bold', color='#34495e')\n",
        "\n",
        "# 5. Styling and labels\n",
        "plt.xlabel('Importance (%)', fontsize=12)\n",
        "plt.title('Feature Importance: Economy Class', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "\n",
        "# Modern cleanup\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "# Add extra space on the right for the percentage labels\n",
        "plt.xlim(0, max(df_econ['Importance_Pct']) + 5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}